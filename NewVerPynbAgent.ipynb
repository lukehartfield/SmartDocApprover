{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogueTex/StreamingDataforModelTraining/blob/main/NewVerPynbAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39e5110",
      "metadata": {
        "id": "c39e5110"
      },
      "source": [
        "# Receipt Automation System\n",
        "\n",
        "This notebook builds a receipt processing pipeline with:\n",
        "- **Document Classification** - ViT model\n",
        "- **OCR** - EasyOCR for text extraction\n",
        "- **Field Extraction** - LayoutLMv3 + regex patterns\n",
        "- **Anomaly Detection** - Isolation Forest\n",
        "- **Agent Workflow** - LangGraph\n",
        "- **Demo UI** - Gradio\n",
        "\n",
        "**Note:** GPU recommended but CPU works too (just slower)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd747a9d",
      "metadata": {
        "id": "bd747a9d"
      },
      "source": [
        "## Setup & Imports\n",
        "Install packages and import stuff we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fdaca231",
      "metadata": {
        "id": "fdaca231",
        "outputId": "424230a2-fe38-404c-a8da-ec52a6a2f32b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hGPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q transformers datasets easyocr langchain langgraph streamlit\n",
        "!pip install -q pillow opencv-python scikit-learn pandas numpy\n",
        "!pip install -q accelerate bitsandbytes\n",
        "!pip install -q albumentations\n",
        "\n",
        "# Check if we have GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU, using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "68da6fc4",
      "metadata": {
        "id": "68da6fc4",
        "outputId": "5416c313-f495-44a5-e747-7ac52ed5f5b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models directory: /Users/shruthisubramanian/Downloads/models\n",
            "Missing models (will be created during training):\n",
            "  rvl_classifier.pt - ViT Document Classifier (~21 MB)\n",
            "  layoutlm_extractor.pt - LayoutLMv3 Field Extractor (~478 MB)\n",
            "  anomaly_detector.pt - Anomaly Detector (~2 MB)\n"
          ]
        }
      ],
      "source": [
        "# Load saved models if you have them\n",
        "# This lets you skip training if you already have .pt files\n",
        "\n",
        "import os\n",
        "\n",
        "# Use absolute path for local storage\n",
        "MODELS_DIR = '/Users/shruthisubramanian/Downloads/models'\n",
        "DATA_DIR = '/Users/shruthisubramanian/Downloads/data'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(DATA_DIR, 'synthetic'), exist_ok=True)\n",
        "\n",
        "MODEL_FILES = {\n",
        "    'rvl_classifier.pt': 'ViT Document Classifier (~21 MB)',\n",
        "    'layoutlm_extractor.pt': 'LayoutLMv3 Field Extractor (~478 MB)',\n",
        "    'anomaly_detector.pt': 'Anomaly Detector (~2 MB)'\n",
        "}\n",
        "\n",
        "# Check what models we have\n",
        "existing_models = []\n",
        "missing_models = []\n",
        "\n",
        "for filename, description in MODEL_FILES.items():\n",
        "    local_path = os.path.join(MODELS_DIR, filename)\n",
        "    if os.path.exists(local_path):\n",
        "        size_mb = os.path.getsize(local_path) / (1024*1024)\n",
        "        existing_models.append((filename, size_mb))\n",
        "    else:\n",
        "        missing_models.append((filename, description))\n",
        "\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n",
        "if existing_models:\n",
        "    print(\"Found models:\")\n",
        "    for name, size in existing_models:\n",
        "        print(f\"  {name} ({size:.1f} MB)\")\n",
        "\n",
        "if missing_models:\n",
        "    print(\"Missing models (will be created during training):\")\n",
        "    for name, desc in missing_models:\n",
        "        print(f\"  {name} - {desc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bd2e74",
      "metadata": {
        "id": "36bd2e74"
      },
      "outputs": [],
      "source": [
        "# All our imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import (\n",
        "    ViTForImageClassification,\n",
        "    ViTImageProcessor,\n",
        "    LayoutLMv3ForTokenClassification,\n",
        "    LayoutLMv3Processor,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import easyocr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Dict, Any, List, Optional\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import albumentations for augmentation\n",
        "try:\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    ALBUMENTATIONS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ALBUMENTATIONS_AVAILABLE = False\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Settings\n",
        "CONFIG = {\n",
        "    # Data settings\n",
        "    'num_synthetic_receipts': 200,\n",
        "    'real_data_samples': 500,\n",
        "\n",
        "    # Model settings - using ViT-Tiny for speed\n",
        "    'vit_model': 'WinKawaks/vit-tiny-patch16-224',\n",
        "    'vit_epochs': 3,\n",
        "    'vit_lr': 3e-4,\n",
        "\n",
        "    # LayoutLM settings\n",
        "    'layoutlm_epochs': 2,\n",
        "    'layoutlm_lr': 5e-5,\n",
        "    'layoutlm_train_samples': 50,\n",
        "\n",
        "    # Training settings\n",
        "    'batch_size': 32,\n",
        "    'early_stopping_patience': 2,\n",
        "    'augmentation_probability': 0.3,\n",
        "    'class_weight_receipt': 1.5,\n",
        "    'warmup_ratio': 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eed2a43",
      "metadata": {
        "id": "5eed2a43"
      },
      "source": [
        "## Data Prep\n",
        "Load receipt datasets (CORD, FUNSD) and make some fake receipts for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a3cbb6",
      "metadata": {
        "id": "34a3cbb6"
      },
      "outputs": [],
      "source": [
        "# Load datasets from HuggingFace and cache them locally\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Dataset cache directory\n",
        "DATASET_CACHE_DIR = Path(\"data/dataset_cache\")\n",
        "DATASET_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# RVL-CDIP has 16 doc types - we only care about receipts/invoices\n",
        "RVL_LABELS = {\n",
        "    0: 'letter', 1: 'form', 2: 'email', 3: 'handwritten',\n",
        "    4: 'advertisement', 5: 'scientific_report', 6: 'scientific_publication',\n",
        "    7: 'specification', 8: 'file_folder', 9: 'news_article',\n",
        "    10: 'budget', 11: 'invoice', 12: 'presentation', 13: 'questionnaire',\n",
        "    14: 'resume', 15: 'memo'\n",
        "}\n",
        "\n",
        "RECEIPT_LABELS = [10, 11]\n",
        "RECEIPT_LABEL_NAMES = ['budget', 'invoice']\n",
        "\n",
        "loaded_datasets = {}\n",
        "\n",
        "def check_cached_dataset(name):\n",
        "    \"\"\"See if we already downloaded this one\"\"\"\n",
        "    cache_file = DATASET_CACHE_DIR / f\"{name}_cache.pkl\"\n",
        "    return cache_file.exists()\n",
        "\n",
        "def save_dataset_cache(name, train_data, val_data, metadata=None):\n",
        "    \"\"\"Save dataset locally so we don't have to download again\"\"\"\n",
        "    cache_file = DATASET_CACHE_DIR / f\"{name}_cache.pkl\"\n",
        "    cache_data = {\n",
        "        'train': train_data,\n",
        "        'val': val_data,\n",
        "        'metadata': metadata or {}\n",
        "    }\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cache_data, f)\n",
        "\n",
        "def load_dataset_cache(name):\n",
        "    \"\"\"Load from local cache\"\"\"\n",
        "    cache_file = DATASET_CACHE_DIR / f\"{name}_cache.pkl\"\n",
        "    with open(cache_file, 'rb') as f:\n",
        "        cache_data = pickle.load(f)\n",
        "    return cache_data['train'], cache_data['val'], cache_data.get('metadata', {})\n",
        "\n",
        "# Initialize\n",
        "dataset = None\n",
        "val_dataset = None\n",
        "use_synthetic_for_classification = False\n",
        "real_images = []\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# CORD - Receipt dataset\n",
        "cord_train, cord_val = None, None\n",
        "if check_cached_dataset(\"cord\"):\n",
        "    try:\n",
        "        cord_train, cord_val, _ = load_dataset_cache(\"cord\")\n",
        "        loaded_datasets['cord'] = {'train': len(cord_train), 'val': len(cord_val)}\n",
        "    except Exception as e:\n",
        "        cord_train, cord_val = None, None\n",
        "\n",
        "if cord_train is None:\n",
        "    try:\n",
        "        cord_train = load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\")\n",
        "        cord_val = load_dataset(\"naver-clova-ix/cord-v2\", split=\"validation\")\n",
        "\n",
        "        def add_cord_label(example):\n",
        "            example['label'] = 1\n",
        "            example['dataset_source'] = 'cord'\n",
        "            return example\n",
        "\n",
        "        cord_train = cord_train.map(add_cord_label)\n",
        "        cord_val = cord_val.map(add_cord_label)\n",
        "        save_dataset_cache(\"cord\", cord_train, cord_val, {'type': 'receipt'})\n",
        "        loaded_datasets['cord'] = {'train': len(cord_train), 'val': len(cord_val)}\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "# SROIE - More receipt data\n",
        "sroie_train, sroie_val = None, None\n",
        "if check_cached_dataset(\"sroie\"):\n",
        "    try:\n",
        "        sroie_train, sroie_val, _ = load_dataset_cache(\"sroie\")\n",
        "        loaded_datasets['sroie'] = {'train': len(sroie_train), 'val': len(sroie_val)}\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "if sroie_train is None:\n",
        "    try:\n",
        "        sroie_full = load_dataset(\"darentang/sroie\", split=\"train\", trust_remote_code=True)\n",
        "        sroie_split = sroie_full.train_test_split(test_size=0.15, seed=42)\n",
        "        sroie_train = sroie_split['train']\n",
        "        sroie_val = sroie_split['test']\n",
        "\n",
        "        def add_sroie_label(example):\n",
        "            example['label'] = 1\n",
        "            example['dataset_source'] = 'sroie'\n",
        "            return example\n",
        "\n",
        "        sroie_train = sroie_train.map(add_sroie_label)\n",
        "        sroie_val = sroie_val.map(add_sroie_label)\n",
        "        save_dataset_cache(\"sroie\", sroie_train, sroie_val, {'type': 'receipt'})\n",
        "        loaded_datasets['sroie'] = {'train': len(sroie_train), 'val': len(sroie_val)}\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "# FUNSD - Form data (NOT receipts, for balance)\n",
        "funsd_train, funsd_val = None, None\n",
        "if check_cached_dataset(\"funsd\"):\n",
        "    try:\n",
        "        funsd_train, funsd_val, _ = load_dataset_cache(\"funsd\")\n",
        "        loaded_datasets['funsd'] = {'train': len(funsd_train), 'val': len(funsd_val)}\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "if funsd_train is None:\n",
        "    try:\n",
        "        funsd_train = load_dataset(\"nielsr/funsd\", split=\"train\", trust_remote_code=True)\n",
        "        funsd_val = load_dataset(\"nielsr/funsd\", split=\"test\", trust_remote_code=True)\n",
        "\n",
        "        def add_funsd_label(example):\n",
        "            example['label'] = 0\n",
        "            example['dataset_source'] = 'funsd'\n",
        "            return example\n",
        "\n",
        "        funsd_train = funsd_train.map(add_funsd_label)\n",
        "        funsd_val = funsd_val.map(add_funsd_label)\n",
        "        save_dataset_cache(\"funsd\", funsd_train, funsd_val, {'type': 'form'})\n",
        "        loaded_datasets['funsd'] = {'train': len(funsd_train), 'val': len(funsd_val)}\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "# RVL-CDIP - Big document dataset (optional)\n",
        "rvl_train, rvl_val = None, None\n",
        "LOAD_RVL_CDIP = False\n",
        "\n",
        "if LOAD_RVL_CDIP:\n",
        "    if check_cached_dataset(\"rvl_cdip\"):\n",
        "        try:\n",
        "            rvl_train, rvl_val, _ = load_dataset_cache(\"rvl_cdip\")\n",
        "            loaded_datasets['rvl_cdip'] = {'train': len(rvl_train), 'val': len(rvl_val)}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    if rvl_train is None:\n",
        "        try:\n",
        "            rvl_train = load_dataset(\"aharley/rvl_cdip\", split=\"train\", trust_remote_code=True)\n",
        "            rvl_val = load_dataset(\"aharley/rvl_cdip\", split=\"test\", trust_remote_code=True)\n",
        "\n",
        "            num_samples = min(CONFIG['real_data_samples'], len(rvl_train))\n",
        "            rvl_train = rvl_train.shuffle(seed=42).select(range(num_samples))\n",
        "            rvl_val = rvl_val.shuffle(seed=42).select(range(num_samples // 4))\n",
        "\n",
        "            def map_rvl_label(example):\n",
        "                example['original_label'] = example['label']\n",
        "                example['label'] = 1 if example['label'] in [10, 11] else 0\n",
        "                example['dataset_source'] = 'rvl_cdip'\n",
        "                return example\n",
        "\n",
        "            rvl_train = rvl_train.map(map_rvl_label)\n",
        "            rvl_val = rvl_val.map(map_rvl_label)\n",
        "            save_dataset_cache(\"rvl_cdip\", rvl_train, rvl_val, {'type': 'mixed'})\n",
        "            loaded_datasets['rvl_cdip'] = {'train': len(rvl_train), 'val': len(rvl_val)}\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "# Combine datasets\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "if cord_train is not None:\n",
        "    train_datasets.append(('cord', cord_train, 1))\n",
        "    val_datasets.append(('cord', cord_val, 1))\n",
        "\n",
        "if sroie_train is not None:\n",
        "    train_datasets.append(('sroie', sroie_train, 1))\n",
        "    val_datasets.append(('sroie', sroie_val, 1))\n",
        "\n",
        "if funsd_train is not None:\n",
        "    train_datasets.append(('funsd', funsd_train, 0))\n",
        "    val_datasets.append(('funsd', funsd_val, 0))\n",
        "\n",
        "if rvl_train is not None:\n",
        "    train_datasets.append(('rvl_cdip', rvl_train, 'mixed'))\n",
        "    val_datasets.append(('rvl_cdip', rvl_val, 'mixed'))\n",
        "\n",
        "# Print summary\n",
        "for name, ds, label_type in train_datasets:\n",
        "    count = len(ds)\n",
        "    if label_type == 1:\n",
        "        print(f\"{name.upper()}: {count} samples (receipts)\")\n",
        "    elif label_type == 0:\n",
        "        print(f\"{name.upper()}: {count} samples (non-receipts)\")\n",
        "    else:\n",
        "        print(f\"{name.upper()}: {count} samples (mixed)\")\n",
        "\n",
        "# Create combined dataset if we have data\n",
        "if train_datasets:\n",
        "    if cord_train is not None:\n",
        "        dataset = cord_train\n",
        "        val_dataset = cord_val\n",
        "    else:\n",
        "        dataset = train_datasets[0][1]\n",
        "        val_dataset = val_datasets[0][1]\n",
        "\n",
        "    if funsd_train is not None:\n",
        "        use_synthetic_for_classification = False\n",
        "    else:\n",
        "        use_synthetic_for_classification = True\n",
        "else:\n",
        "    use_synthetic_for_classification = True\n",
        "\n",
        "AVAILABLE_DATASETS = {\n",
        "    'cord': (cord_train, cord_val),\n",
        "    'sroie': (sroie_train, sroie_val),\n",
        "    'funsd': (funsd_train, funsd_val),\n",
        "    'rvl_cdip': (rvl_train, rvl_val)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a007d0",
      "metadata": {
        "id": "c8a007d0"
      },
      "outputs": [],
      "source": [
        "# Make fake receipts for training\n",
        "\n",
        "class EnhancedReceiptGenerator:\n",
        "    \"\"\"Creates realistic looking fake receipts.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vendors = [\n",
        "            \"WALMART\", \"TARGET\", \"COSTCO\", \"STARBUCKS\", \"MCDONALD'S\",\n",
        "            \"AMAZON\", \"BEST BUY\", \"HOME DEPOT\", \"WHOLE FOODS\", \"CVS PHARMACY\",\n",
        "            \"WALGREENS\", \"TRADER JOE'S\", \"KROGER\", \"SAFEWAY\", \"7-ELEVEN\",\n",
        "            \"SUBWAY\", \"CHIPOTLE\", \"DOMINO'S\", \"PIZZA HUT\", \"TACO BELL\",\n",
        "            \"WENDY'S\", \"BURGER KING\", \"DUNKIN\", \"PANERA BREAD\", \"CHICK-FIL-A\"\n",
        "        ]\n",
        "\n",
        "        self.items = [\n",
        "            (\"COFFEE REG\", 4.99), (\"SANDWICH TKY\", 8.49), (\"MILK 1GAL\", 3.99),\n",
        "            (\"BREAD WHL WHT\", 2.49), (\"EGGS LARGE 12\", 5.99), (\"CHICKEN BRST\", 12.99),\n",
        "            (\"PASTA PENNE\", 1.99), (\"CHEESE CHEDDR\", 6.49), (\"APPLES FUJI\", 4.49),\n",
        "            (\"ORANGE JUICE\", 5.99), (\"SOAP DISH LIQ\", 3.49), (\"PAPER TOWELS\", 8.99),\n",
        "            (\"AA BATTERIES\", 9.99), (\"HDMI CABLE 6F\", 15.99), (\"USB CHARGER\", 12.99),\n",
        "            (\"WATER 24PK\", 4.99), (\"CHIPS LAYS\", 3.49), (\"SODA 12PK\", 5.99),\n",
        "            (\"YOGURT GREEK\", 1.29), (\"CEREAL CHRIOS\", 4.49), (\"BACON 1LB\", 7.99),\n",
        "            (\"BUTTER UNSALT\", 4.99), (\"LETTUCE ROMN\", 2.99), (\"TOMATOES\", 3.49),\n",
        "            (\"ONIONS YLW\", 1.99), (\"POTATOES 5LB\", 4.99), (\"RICE LONG GR\", 3.99),\n",
        "        ]\n",
        "\n",
        "        self.formats = ['standard', 'minimal', 'detailed', 'wide', 'narrow']\n",
        "        self.fonts = self._load_fonts()\n",
        "\n",
        "    def _load_fonts(self):\n",
        "        \"\"\"Load available system fonts with fallback\"\"\"\n",
        "        font_configs = []\n",
        "        font_paths = [\n",
        "            \"/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf\",\n",
        "            \"/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf\",\n",
        "            \"/usr/share/fonts/truetype/freefont/FreeMono.ttf\",\n",
        "        ]\n",
        "\n",
        "        for path in font_paths:\n",
        "            try:\n",
        "                font = ImageFont.truetype(path, 14)\n",
        "                font_bold = ImageFont.truetype(path.replace('.ttf', '-Bold.ttf').replace('Regular', 'Bold'), 16)\n",
        "                font_configs.append((font, font_bold))\n",
        "            except:\n",
        "                try:\n",
        "                    font = ImageFont.truetype(path, 14)\n",
        "                    font_configs.append((font, font))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if not font_configs:\n",
        "            default = ImageFont.load_default()\n",
        "            font_configs.append((default, default))\n",
        "\n",
        "        return font_configs\n",
        "\n",
        "    def _random_date(self):\n",
        "        \"\"\"Generate random date in various formats\"\"\"\n",
        "        days_ago = random.randint(0, 730)\n",
        "        date = datetime.now() - timedelta(days=days_ago)\n",
        "        formats = [\"%m/%d/%Y\", \"%m/%d/%y\", \"%Y-%m-%d\"]\n",
        "        return date.strftime(random.choice(formats))\n",
        "\n",
        "    def _random_time(self):\n",
        "        \"\"\"Generate random time\"\"\"\n",
        "        hour = random.randint(6, 23)\n",
        "        minute = random.randint(0, 59)\n",
        "\n",
        "        if random.random() > 0.5:\n",
        "            hour_12 = hour if hour <= 12 else hour - 12\n",
        "            hour_12 = 12 if hour_12 == 0 else hour_12\n",
        "            period = \"AM\" if hour < 12 else \"PM\"\n",
        "            return f\"{hour_12}:{minute:02d} {period}\"\n",
        "        else:\n",
        "            return f\"{hour:02d}:{minute:02d}\"\n",
        "\n",
        "    def generate_receipt(self, format_type=None, add_noise=True, add_wrinkles=True, save_path=None):\n",
        "        \"\"\"Generate a synthetic receipt with realistic variations\"\"\"\n",
        "        format_type = format_type or random.choice(self.formats)\n",
        "        font, font_bold = random.choice(self.fonts)\n",
        "\n",
        "        # Variable dimensions based on format\n",
        "        if format_type == 'narrow':\n",
        "            width = random.randint(280, 320)\n",
        "            height = random.randint(500, 700)\n",
        "        elif format_type == 'wide':\n",
        "            width = random.randint(450, 500)\n",
        "            height = random.randint(400, 550)\n",
        "        else:\n",
        "            width = random.randint(350, 420)\n",
        "            height = random.randint(500, 750)\n",
        "\n",
        "        # Background color\n",
        "        bg_value = random.randint(245, 255)\n",
        "        bg_color = (bg_value, bg_value, random.randint(bg_value-5, bg_value))\n",
        "        img = Image.new('RGB', (width, height), color=bg_color)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "\n",
        "        # Text color\n",
        "        text_value = random.randint(0, 40)\n",
        "        text_color = (text_value, text_value, text_value)\n",
        "\n",
        "        # Generate receipt content\n",
        "        vendor = random.choice(self.vendors)\n",
        "        date = self._random_date()\n",
        "        time = self._random_time()\n",
        "\n",
        "        num_items = random.randint(2, min(12, len(self.items)))\n",
        "        selected_items = random.sample(self.items, num_items)\n",
        "\n",
        "        receipt_items = []\n",
        "        subtotal = 0\n",
        "        for name, base_price in selected_items:\n",
        "            qty = random.randint(1, 4)\n",
        "            price = round(base_price * random.uniform(0.85, 1.15), 2)\n",
        "            total = round(price * qty, 2)\n",
        "            subtotal += total\n",
        "            receipt_items.append((name, qty, price, total))\n",
        "\n",
        "        tax_rate = random.choice([0.0, 0.04, 0.0625, 0.0725, 0.0825, 0.095, 0.10])\n",
        "        tax = round(subtotal * tax_rate, 2)\n",
        "        total = round(subtotal + tax, 2)\n",
        "\n",
        "        # Draw receipt\n",
        "        y_pos = random.randint(15, 30)\n",
        "\n",
        "        # Header\n",
        "        vendor_x = (width - len(vendor) * 8) // 2\n",
        "        draw.text((vendor_x, y_pos), vendor, fill=text_color, font=font_bold)\n",
        "        y_pos += 35\n",
        "\n",
        "        # Address (sometimes)\n",
        "        if format_type in ['detailed', 'standard'] and random.random() > 0.5:\n",
        "            address = f\"{random.randint(100, 9999)} {random.choice(['MAIN', 'OAK', 'ELM', 'PARK'])} ST\"\n",
        "            draw.text((20, y_pos), address, fill=text_color, font=font)\n",
        "            y_pos += 20\n",
        "\n",
        "        # Date and time\n",
        "        if format_type == 'minimal':\n",
        "            draw.text((20, y_pos), f\"{date}\", fill=text_color, font=font)\n",
        "        else:\n",
        "            draw.text((20, y_pos), f\"Date: {date} Time: {time}\", fill=text_color, font=font)\n",
        "        y_pos += 25\n",
        "\n",
        "        # Separator\n",
        "        sep_char = random.choice([\"-\", \"=\", \"*\"])\n",
        "        draw.text((20, y_pos), sep_char * (width // 10), fill=text_color, font=font)\n",
        "        y_pos += 20\n",
        "\n",
        "        # Items\n",
        "        for name, qty, price, item_total in receipt_items:\n",
        "            if format_type == 'minimal':\n",
        "                line = f\"{name} ${item_total:.2f}\"\n",
        "            elif format_type == 'detailed':\n",
        "                draw.text((20, y_pos), name, fill=text_color, font=font)\n",
        "                y_pos += 18\n",
        "                line = f\" {qty} @ ${price:.2f} = ${item_total:.2f}\"\n",
        "            else:\n",
        "                line = f\"{name:<16} {qty}x${price:.2f} ${item_total:.2f}\"\n",
        "\n",
        "            draw.text((20, y_pos), line, fill=text_color, font=font)\n",
        "            y_pos += 20\n",
        "\n",
        "        # Separator\n",
        "        y_pos += 5\n",
        "        draw.text((20, y_pos), sep_char * (width // 10), fill=text_color, font=font)\n",
        "        y_pos += 20\n",
        "\n",
        "        # Totals\n",
        "        draw.text((20, y_pos), f\"SUBTOTAL:{' ' * 10}${subtotal:.2f}\", fill=text_color, font=font)\n",
        "        y_pos += 20\n",
        "\n",
        "        if tax_rate > 0:\n",
        "            tax_pct = f\"({tax_rate*100:.2f}%)\" if format_type == 'detailed' else \"\"\n",
        "            draw.text((20, y_pos), f\"TAX {tax_pct}:{' ' * 8}${tax:.2f}\", fill=text_color, font=font)\n",
        "            y_pos += 20\n",
        "\n",
        "        draw.text((20, y_pos), f\"TOTAL:{' ' * 12}${total:.2f}\", fill=text_color, font=font_bold)\n",
        "        y_pos += 30\n",
        "\n",
        "        # Footer\n",
        "        footers = [\"Thank you!\", \"THANK YOU FOR SHOPPING!\", \"Have a nice day!\", \"Please come again\", \"Save this receipt\"]\n",
        "        footer = random.choice(footers)\n",
        "        footer_x = (width - len(footer) * 7) // 2\n",
        "        draw.text((footer_x, y_pos), footer, fill=text_color, font=font)\n",
        "\n",
        "        # Add realistic artifacts\n",
        "        if add_noise:\n",
        "            img = self._add_noise(img)\n",
        "        if add_wrinkles:\n",
        "            img = self._add_wrinkles(img)\n",
        "\n",
        "        # Random rotation\n",
        "        if random.random() > 0.7:\n",
        "            angle = random.uniform(-3, 3)\n",
        "            img = img.rotate(angle, fillcolor=bg_color, expand=False)\n",
        "\n",
        "        ground_truth = {\n",
        "            'vendor': vendor,\n",
        "            'date': date,\n",
        "            'time': time,\n",
        "            'items': receipt_items,\n",
        "            'subtotal': subtotal,\n",
        "            'tax': tax,\n",
        "            'total': total,\n",
        "            'tax_rate': tax_rate,\n",
        "            'format': format_type,\n",
        "            'num_items': len(receipt_items)\n",
        "        }\n",
        "\n",
        "        if save_path:\n",
        "            img.save(save_path)\n",
        "\n",
        "        return img, ground_truth\n",
        "\n",
        "    def _add_noise(self, img, intensity=None):\n",
        "        \"\"\"Add some random noise to make it look scanned\"\"\"\n",
        "        intensity = intensity or random.uniform(2, 10)\n",
        "        arr = np.array(img, dtype=np.float32)\n",
        "        noise = np.random.normal(0, intensity, arr.shape)\n",
        "        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
        "        return Image.fromarray(arr)\n",
        "\n",
        "    def _add_wrinkles(self, img):\n",
        "        \"\"\"Add some fold lines and shadows\"\"\"\n",
        "        arr = np.array(img, dtype=np.float32)\n",
        "        h, w = arr.shape[:2]\n",
        "\n",
        "        num_folds = random.randint(0, 3)\n",
        "        for _ in range(num_folds):\n",
        "            if random.random() > 0.5:\n",
        "                y = random.randint(h // 5, 4 * h // 5)\n",
        "                thickness = random.randint(1, 3)\n",
        "                darkness = random.uniform(0.85, 0.95)\n",
        "                arr[y-thickness:y+thickness, :] *= darkness\n",
        "            else:\n",
        "                x = random.randint(w // 5, 4 * w // 5)\n",
        "                thickness = random.randint(1, 3)\n",
        "                darkness = random.uniform(0.85, 0.95)\n",
        "                arr[:, x-thickness:x+thickness] *= darkness\n",
        "\n",
        "        if random.random() > 0.6:\n",
        "            shadow_width = random.randint(5, 15)\n",
        "            shadow_strength = random.uniform(0.9, 0.98)\n",
        "            arr[:, :shadow_width] *= shadow_strength\n",
        "            arr[:, -shadow_width:] *= shadow_strength\n",
        "\n",
        "        return Image.fromarray(np.clip(arr, 0, 255).astype(np.uint8))\n",
        "\n",
        "    def generate_batch(self, num_samples, save_dir=None):\n",
        "        \"\"\"Make a bunch of fake receipts\"\"\"\n",
        "        receipts = []\n",
        "        ground_truths = []\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            save_path = f\"{save_dir}/receipt_{i:04d}.png\" if save_dir else None\n",
        "            img, gt = self.generate_receipt(save_path=save_path)\n",
        "            receipts.append(img)\n",
        "            ground_truths.append(gt)\n",
        "\n",
        "        return receipts, ground_truths\n",
        "\n",
        "# Make the fake receipts\n",
        "generator = EnhancedReceiptGenerator()\n",
        "\n",
        "synthetic_receipts, synthetic_ground_truth = generator.generate_batch(\n",
        "    num_samples=CONFIG['num_synthetic_receipts'],\n",
        "    save_dir=None\n",
        ")\n",
        "\n",
        "# Show sample stats\n",
        "formats_used = {}\n",
        "for gt in synthetic_ground_truth:\n",
        "    fmt = gt['format']\n",
        "    formats_used[fmt] = formats_used.get(fmt, 0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0e1aba",
      "metadata": {
        "id": "5e0e1aba"
      },
      "outputs": [],
      "source": [
        "# Data augmentation - mess up images a bit so model learns better\n",
        "\n",
        "try:\n",
        "    import albumentations as A\n",
        "    ALBUMENTATIONS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ALBUMENTATIONS_AVAILABLE = False\n",
        "\n",
        "class ReceiptAugmentation:\n",
        "    \"\"\"Messes up images in realistic ways - rotation, blur, shadows, etc.\"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "        if ALBUMENTATIONS_AVAILABLE:\n",
        "            self.transform = A.Compose([\n",
        "                A.OneOf([\n",
        "                    A.Rotate(limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=(255, 255, 255)),\n",
        "                    A.Perspective(scale=(0.02, 0.05), p=0.3),\n",
        "                    A.Affine(shear=(-10, 10), p=0.3, border_mode=cv2.BORDER_CONSTANT, cval=(255, 255, 255)),\n",
        "                ], p=0.5),\n",
        "                A.OneOf([\n",
        "                    A.GaussNoise(var_limit=(10, 50), p=0.4),\n",
        "                    A.ISONoise(color_shift=(0.01, 0.05), p=0.3),\n",
        "                    A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=0.3),\n",
        "                ], p=0.4),\n",
        "                A.OneOf([\n",
        "                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "                    A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
        "                    A.CLAHE(clip_limit=2.0, p=0.3),\n",
        "                ], p=0.5),\n",
        "                A.OneOf([\n",
        "                    A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
        "                    A.MotionBlur(blur_limit=3, p=0.2),\n",
        "                    A.MedianBlur(blur_limit=3, p=0.2),\n",
        "                ], p=0.3),\n",
        "                A.OneOf([\n",
        "                    A.RandomShadow(shadow_roi=(0, 0, 1, 1), p=0.2),\n",
        "                    A.CoarseDropout(max_holes=5, max_height=15, max_width=15, fill_value=220, p=0.2),\n",
        "                ], p=0.2),\n",
        "                A.OneOf([\n",
        "                    A.ToGray(p=0.3),\n",
        "                    A.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n",
        "                ], p=0.2),\n",
        "                A.Resize(224, 224),\n",
        "            ])\n",
        "            self.val_transform = A.Compose([A.Resize(224, 224)])\n",
        "        else:\n",
        "            self.transform = None\n",
        "            self.val_transform = None\n",
        "\n",
        "    def __call__(self, image, is_training=True):\n",
        "        \"\"\"Run the augmentation on an image\"\"\"\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        if is_training and self.transform is not None:\n",
        "            augmented = self.transform(image=image)\n",
        "            return augmented['image']\n",
        "        elif self.val_transform is not None:\n",
        "            augmented = self.val_transform(image=image)\n",
        "            return augmented['image']\n",
        "        else:\n",
        "            img = Image.fromarray(image) if isinstance(image, np.ndarray) else image\n",
        "            return np.array(img.resize((224, 224)))\n",
        "\n",
        "RECEIPT_LABELS = {1}\n",
        "\n",
        "class AugmentedReceiptDataset(Dataset):\n",
        "    \"\"\"Wraps our data for PyTorch training.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, processor, augmentation=None, is_training=True, is_receipt_labels=None):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "        self.augmentation = augmentation\n",
        "        self.is_training = is_training\n",
        "        self.is_receipt_labels = is_receipt_labels if is_receipt_labels is not None else RECEIPT_LABELS\n",
        "        self.mean = np.array([0.485, 0.456, 0.406])\n",
        "        self.std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        image = item['image']\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            image_np = self.augmentation(image, is_training=self.is_training)\n",
        "        else:\n",
        "            image_np = np.array(image.resize((224, 224)))\n",
        "\n",
        "        image_np = image_np.astype(np.float32) / 255.0\n",
        "        image_np = (image_np - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float()\n",
        "        label = 1 if item['label'] in self.is_receipt_labels else 0\n",
        "\n",
        "        return {\n",
        "            'pixel_values': image_tensor,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Set up augmentation\n",
        "receipt_augmentation = ReceiptAugmentation(p=CONFIG['augmentation_probability'])\n",
        "\n",
        "try:\n",
        "    vit_processor = ViTImageProcessor.from_pretrained(CONFIG.get('vit_model', 'WinKawaks/vit-tiny-patch16-224'))\n",
        "except:\n",
        "    vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "class SyntheticReceiptDataset(Dataset):\n",
        "    \"\"\"Dataset for when we only have fake receipts.\"\"\"\n",
        "\n",
        "    def __init__(self, receipts, ground_truths, augmentation=None, is_training=True, include_negatives=True):\n",
        "        self.receipts = receipts\n",
        "        self.ground_truths = ground_truths\n",
        "        self.augmentation = augmentation\n",
        "        self.is_training = is_training\n",
        "        self.samples = []\n",
        "\n",
        "        for i, (img, gt) in enumerate(zip(receipts, ground_truths)):\n",
        "            self.samples.append({'image': img, 'label': 1, 'ground_truth': gt})\n",
        "\n",
        "        if include_negatives:\n",
        "            num_negatives = len(receipts) // 3\n",
        "            for i in range(num_negatives):\n",
        "                neg_img = self._generate_non_receipt()\n",
        "                self.samples.append({'image': neg_img, 'label': 0, 'ground_truth': None})\n",
        "\n",
        "        self.mean = np.array([0.485, 0.456, 0.406])\n",
        "        self.std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "        print(f\"Created dataset with {sum(1 for s in self.samples if s['label']==1)} receipts, \"\n",
        "              f\"{sum(1 for s in self.samples if s['label']==0)} non-receipts\")\n",
        "\n",
        "    def _generate_non_receipt(self):\n",
        "        \"\"\"Make a random non-receipt image\"\"\"\n",
        "        width, height = 400, 600\n",
        "        img_type = random.choice(['blank', 'noise', 'shapes', 'text'])\n",
        "\n",
        "        if img_type == 'blank':\n",
        "            color = tuple(random.randint(180, 255) for _ in range(3))\n",
        "            img = Image.new('RGB', (width, height), color=color)\n",
        "        elif img_type == 'noise':\n",
        "            arr = np.random.randint(150, 255, (height, width, 3), dtype=np.uint8)\n",
        "            img = Image.fromarray(arr)\n",
        "        elif img_type == 'shapes':\n",
        "            img = Image.new('RGB', (width, height), color='white')\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            for _ in range(random.randint(3, 10)):\n",
        "                shape = random.choice(['rectangle', 'ellipse', 'line'])\n",
        "                color = tuple(random.randint(0, 200) for _ in range(3))\n",
        "                x1, y1 = random.randint(0, width), random.randint(0, height)\n",
        "                x2, y2 = random.randint(0, width), random.randint(0, height)\n",
        "                if shape == 'rectangle':\n",
        "                    draw.rectangle([min(x1,x2), min(y1,y2), max(x1,x2), max(y1,y2)], outline=color)\n",
        "                elif shape == 'ellipse':\n",
        "                    draw.ellipse([min(x1,x2), min(y1,y2), max(x1,x2), max(y1,y2)], outline=color)\n",
        "                else:\n",
        "                    draw.line([x1, y1, x2, y2], fill=color, width=2)\n",
        "        else:\n",
        "            img = Image.new('RGB', (width, height), color='white')\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            try:\n",
        "                font = ImageFont.load_default()\n",
        "            except:\n",
        "                font = None\n",
        "            words = [\"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"document\", \"page\", \"file\"]\n",
        "            for _ in range(random.randint(5, 15)):\n",
        "                text = \" \".join(random.choices(words, k=random.randint(2, 6)))\n",
        "                x = random.randint(10, width - 100)\n",
        "                y = random.randint(10, height - 30)\n",
        "                draw.text((x, y), text, fill='black', font=font)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        image = sample['image']\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            image_np = self.augmentation(image, is_training=self.is_training)\n",
        "        else:\n",
        "            image_np = np.array(image.resize((224, 224)))\n",
        "\n",
        "        image_np = image_np.astype(np.float32) / 255.0\n",
        "        image_np = (image_np - self.mean) / self.std\n",
        "        image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float()\n",
        "\n",
        "        return {\n",
        "            'pixel_values': image_tensor,\n",
        "            'labels': torch.tensor(sample['label'], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6125cee",
      "metadata": {
        "id": "c6125cee"
      },
      "source": [
        "## ViT Classifier\n",
        "Train a Vision Transformer to tell receipts from other docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef9aa9b",
      "metadata": {
        "id": "7ef9aa9b"
      },
      "outputs": [],
      "source": [
        "# ViT document classifier\n",
        "\n",
        "class DocumentClassifier:\n",
        "    \"\"\"Uses ViT-Tiny to classify docs as receipt or not.\"\"\"\n",
        "\n",
        "    def __init__(self, num_labels=2, pretrained=None):\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained = pretrained or CONFIG.get('vit_model', 'WinKawaks/vit-tiny-patch16-224')\n",
        "        self.model = None\n",
        "        self.processor = None\n",
        "        self.best_val_acc = 0\n",
        "        self.model_path = os.path.join(MODELS_DIR, 'rvl_classifier.pt')\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the pretrained ViT and set it up for 2-class output\"\"\"\n",
        "        try:\n",
        "            self.processor = ViTImageProcessor.from_pretrained(self.pretrained)\n",
        "        except:\n",
        "            self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        self.model = ViTForImageClassification.from_pretrained(\n",
        "            self.pretrained,\n",
        "            num_labels=self.num_labels,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        return self.model\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=None, lr=None, class_weights=None,\n",
        "              warmup_ratio=None, patience=None, weight_decay=0.01, max_grad_norm=1.0):\n",
        "        \"\"\"Train the model with early stopping\"\"\"\n",
        "        epochs = epochs or CONFIG['vit_epochs']\n",
        "        lr = lr or CONFIG['vit_lr']\n",
        "        warmup_ratio = warmup_ratio or CONFIG['warmup_ratio']\n",
        "        patience = patience or CONFIG['early_stopping_patience']\n",
        "\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        total_steps = len(train_loader) * epochs\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer, max_lr=lr * 10, total_steps=total_steps,\n",
        "            pct_start=warmup_ratio, anneal_strategy='cos',\n",
        "            div_factor=25, final_div_factor=1000\n",
        "        )\n",
        "\n",
        "        if class_weights is not None:\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "        else:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        use_amp = torch.cuda.is_available()\n",
        "        scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
        "\n",
        "        self.best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_loader):\n",
        "                pixel_values = batch['pixel_values'].to(DEVICE)\n",
        "                labels = batch['labels'].to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = self.model(pixel_values=pixel_values)\n",
        "                        loss = criterion(outputs.logits, labels)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.unscale_(optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    outputs = self.model(pixel_values=pixel_values)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
        "                    optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.logits.max(1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            val_loss, val_acc = self.evaluate(val_loader, criterion)\n",
        "            train_acc = 100 * train_correct / train_total\n",
        "\n",
        "            history['train_loss'].append(train_loss / len(train_loader))\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['lr'].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "            if val_acc > self.best_val_acc:\n",
        "                self.best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                self.save_model(self.model_path)\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                break\n",
        "\n",
        "        self.load_weights(self.model_path)\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, val_loader, criterion=None):\n",
        "        \"\"\"Check how well we're doing on val data\"\"\"\n",
        "        if criterion is None:\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                pixel_values = batch['pixel_values'].to(DEVICE)\n",
        "                labels = batch['labels'].to(DEVICE)\n",
        "                outputs = self.model(pixel_values=pixel_values)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.logits.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        return val_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "    def predict(self, image):\n",
        "        \"\"\"Check if an image is a receipt\"\"\"\n",
        "        self.model.eval()\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(pixel_values=pixel_values)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            receipt_prob = probs[0][1].item()\n",
        "\n",
        "        return {\n",
        "            'is_receipt': receipt_prob > 0.5,\n",
        "            'confidence': receipt_prob,\n",
        "            'label': 'receipt' if receipt_prob > 0.5 else 'other'\n",
        "        }\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Model saved to: {path}\")\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        \"\"\"Load model weights\"\"\"\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "        self.model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "        self.model.eval()\n",
        "        print(f\"Model loaded from: {path}\")\n",
        "\n",
        "# Initialize classifier\n",
        "doc_classifier = DocumentClassifier(num_labels=2)\n",
        "doc_classifier.load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc3901c",
      "metadata": {
        "id": "8bc3901c"
      },
      "outputs": [],
      "source": [
        "# Train the classifier (or load if we already have it)\n",
        "\n",
        "VIT_MODEL_PATH = os.path.join(MODELS_DIR, 'rvl_classifier.pt')\n",
        "SKIP_TRAINING_IF_EXISTS = True\n",
        "\n",
        "# Create data loaders if we have data\n",
        "if dataset is not None:\n",
        "    train_dataset = AugmentedReceiptDataset(dataset, vit_processor, receipt_augmentation, is_training=True)\n",
        "    val_dataset_wrapped = AugmentedReceiptDataset(val_dataset, vit_processor, receipt_augmentation, is_training=False)\n",
        "else:\n",
        "    train_dataset = SyntheticReceiptDataset(synthetic_receipts, synthetic_ground_truth, receipt_augmentation, is_training=True)\n",
        "    val_size = len(synthetic_receipts) // 5\n",
        "    val_dataset_wrapped = SyntheticReceiptDataset(synthetic_receipts[:val_size], synthetic_ground_truth[:val_size],\n",
        "                                                   receipt_augmentation, is_training=False, include_negatives=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset_wrapped, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
        "\n",
        "# Class weights for imbalanced data\n",
        "class_weights = torch.tensor([1.0, CONFIG['class_weight_receipt']], dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "# Check if trained model already exists\n",
        "if SKIP_TRAINING_IF_EXISTS and os.path.exists(VIT_MODEL_PATH):\n",
        "    print(f\"Loading model from: {VIT_MODEL_PATH}\")\n",
        "    doc_classifier.load_weights(VIT_MODEL_PATH)\n",
        "    history = None\n",
        "else:\n",
        "    print(f\"Training model, will save to: {VIT_MODEL_PATH}\")\n",
        "    history = doc_classifier.train(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=CONFIG['vit_epochs'],\n",
        "        lr=CONFIG['vit_lr'],\n",
        "        class_weights=class_weights,\n",
        "        warmup_ratio=CONFIG['warmup_ratio'],\n",
        "        patience=CONFIG['early_stopping_patience']\n",
        "    )\n",
        "    doc_classifier.save_model(VIT_MODEL_PATH)\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "# Test on synthetic receipts\n",
        "correct = 0\n",
        "for i in range(min(10, len(synthetic_receipts))):\n",
        "    result = doc_classifier.predict(synthetic_receipts[i])\n",
        "    if result['is_receipt']:\n",
        "        correct += 1\n",
        "    if i < 3:\n",
        "        print(f\"Receipt {i+1}: {result['label']} (confidence: {result['confidence']:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f00b5b",
      "metadata": {
        "id": "59f00b5b"
      },
      "source": [
        "## EasyOCR\n",
        "Set up OCR to read text from receipt images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960a108e",
      "metadata": {
        "id": "960a108e"
      },
      "outputs": [],
      "source": [
        "# OCR wrapper class\n",
        "\n",
        "class ReceiptOCR:\n",
        "    \"\"\"Wrapper around EasyOCR with some receipt-specific tricks\"\"\"\n",
        "\n",
        "    def __init__(self, languages=['en'], gpu=True):\n",
        "        self.reader = easyocr.Reader(languages, gpu=gpu and torch.cuda.is_available())\n",
        "        self.languages = languages\n",
        "\n",
        "    def extract_text(self, image, detail=1):\n",
        "        \"\"\"Pull text out of an image\"\"\"\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "        return self.reader.readtext(image, detail=detail)\n",
        "\n",
        "    def extract_with_positions(self, image):\n",
        "        \"\"\"Get text with bounding boxes\"\"\"\n",
        "        if isinstance(image, Image.Image):\n",
        "            image = np.array(image)\n",
        "\n",
        "        results = self.extract_text(image, detail=1)\n",
        "        extracted = []\n",
        "\n",
        "        for bbox, text, conf in results:\n",
        "            x_center = (bbox[0][0] + bbox[2][0]) / 2\n",
        "            y_center = (bbox[0][1] + bbox[2][1]) / 2\n",
        "            extracted.append({\n",
        "                'text': text,\n",
        "                'confidence': conf,\n",
        "                'bbox': bbox,\n",
        "                'x_center': x_center,\n",
        "                'y_center': y_center,\n",
        "                'width': bbox[2][0] - bbox[0][0],\n",
        "                'height': bbox[2][1] - bbox[0][1]\n",
        "            })\n",
        "\n",
        "        extracted.sort(key=lambda x: x['y_center'])\n",
        "        return extracted\n",
        "\n",
        "    def postprocess_receipt(self, ocr_results):\n",
        "        \"\"\"Try to find vendor, date, total from OCR text\"\"\"\n",
        "        import re\n",
        "        full_text = ' '.join([r['text'] for r in ocr_results])\n",
        "\n",
        "        # Extract date\n",
        "        date_patterns = [\n",
        "            r'\\d{1,2}/\\d{1,2}/\\d{2,4}',\n",
        "            r'\\d{1,2}-\\d{1,2}-\\d{2,4}',\n",
        "            r'\\d{4}-\\d{2}-\\d{2}',\n",
        "        ]\n",
        "        date = None\n",
        "        for pattern in date_patterns:\n",
        "            match = re.search(pattern, full_text)\n",
        "            if match:\n",
        "                date = match.group()\n",
        "                break\n",
        "\n",
        "        # Extract amounts\n",
        "        amount_pattern = r'\\$?\\d+\\.\\d{2}'\n",
        "        amounts = re.findall(amount_pattern, full_text)\n",
        "        amounts = [float(a.replace('$', '')) for a in amounts]\n",
        "        total = max(amounts) if amounts else 0.0\n",
        "\n",
        "        # Extract vendor\n",
        "        vendor = None\n",
        "        for r in ocr_results[:3]:\n",
        "            text = r['text'].strip()\n",
        "            if len(text) > 2 and text.isupper():\n",
        "                vendor = text\n",
        "                break\n",
        "\n",
        "        # Extract time\n",
        "        time_pattern = r'\\d{1,2}:\\d{2}(?::\\d{2})?(?:\\s*[AP]M)?'\n",
        "        time_match = re.search(time_pattern, full_text, re.IGNORECASE)\n",
        "        time = time_match.group() if time_match else None\n",
        "\n",
        "        return {\n",
        "            'vendor': vendor,\n",
        "            'date': date,\n",
        "            'time': time,\n",
        "            'total': total,\n",
        "            'all_amounts': amounts,\n",
        "            'raw_text': full_text,\n",
        "            'num_lines': len(ocr_results)\n",
        "        }\n",
        "\n",
        "    def visualize_results(self, image, ocr_results):\n",
        "        \"\"\"Draw boxes around detected text\"\"\"\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        img_draw = image.copy()\n",
        "        draw = ImageDraw.Draw(img_draw)\n",
        "\n",
        "        for r in ocr_results:\n",
        "            bbox = r['bbox']\n",
        "            points = [(int(p[0]), int(p[1])) for p in bbox]\n",
        "            draw.polygon(points, outline='red', width=2)\n",
        "            draw.text((points[0][0], points[0][1] - 15),\n",
        "                      f\"{r['text'][:20]} ({r['confidence']:.2f})\", fill='blue')\n",
        "\n",
        "        return img_draw\n",
        "\n",
        "# Initialize OCR\n",
        "receipt_ocr = ReceiptOCR(languages=['en'], gpu=True)\n",
        "\n",
        "# Test on synthetic receipt\n",
        "test_results = receipt_ocr.extract_with_positions(synthetic_receipts[0])\n",
        "extracted_data = receipt_ocr.postprocess_receipt(test_results)\n",
        "print(f\"Vendor: {extracted_data['vendor']}\")\n",
        "print(f\"Date: {extracted_data['date']}\")\n",
        "print(f\"Total: ${extracted_data['total']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09440385",
      "metadata": {
        "id": "09440385"
      },
      "source": [
        "## LayoutLMv3 Field Extractor\n",
        "This model finds vendor, date, and total in receipts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aa6916",
      "metadata": {
        "id": "86aa6916"
      },
      "outputs": [],
      "source": [
        "# LayoutLMv3 for finding fields in receipts\n",
        "\n",
        "FIELD_LABELS = {\n",
        "    'O': 0,\n",
        "    'B-VENDOR': 1,\n",
        "    'I-VENDOR': 2,\n",
        "    'B-DATE': 3,\n",
        "    'I-DATE': 4,\n",
        "    'B-TOTAL': 5,\n",
        "    'I-TOTAL': 6,\n",
        "}\n",
        "NUM_LABELS = len(FIELD_LABELS)\n",
        "\n",
        "LAYOUTLM_MODEL_PATH = os.path.join(MODELS_DIR, 'layoutlm_extractor.pt')\n",
        "\n",
        "class LayoutLMExtractor:\n",
        "    \"\"\"Uses LayoutLMv3 to find vendor/date/total in receipts\"\"\"\n",
        "\n",
        "    def __init__(self, num_labels=NUM_LABELS, pretrained=\"microsoft/layoutlmv3-base\"):\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained = pretrained\n",
        "        self.model = None\n",
        "        self.processor = None\n",
        "        self.label_map = FIELD_LABELS\n",
        "        self.id2label = {v: k for k, v in FIELD_LABELS.items()}\n",
        "        self.model_path = LAYOUTLM_MODEL_PATH\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load LayoutLMv3 from HuggingFace\"\"\"\n",
        "        self.processor = LayoutLMv3Processor.from_pretrained(self.pretrained, apply_ocr=False)\n",
        "        self.model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
        "            self.pretrained,\n",
        "            num_labels=self.num_labels,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        self.model = self.model.to(DEVICE)\n",
        "        return self.model\n",
        "\n",
        "    def prepare_inputs(self, image, ocr_results):\n",
        "        \"\"\"Format image + OCR for LayoutLMv3\"\"\"\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        words = []\n",
        "        boxes = []\n",
        "        width, height = image.size\n",
        "\n",
        "        for r in ocr_results:\n",
        "            text = r['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            bbox = r['bbox']\n",
        "            x0 = int(min(p[0] for p in bbox) * 1000 / width)\n",
        "            y0 = int(min(p[1] for p in bbox) * 1000 / height)\n",
        "            x1 = int(max(p[0] for p in bbox) * 1000 / width)\n",
        "            y1 = int(max(p[1] for p in bbox) * 1000 / height)\n",
        "            x0, y0, x1, y1 = [max(0, min(1000, v)) for v in [x0, y0, x1, y1]]\n",
        "\n",
        "            words.append(text)\n",
        "            boxes.append([x0, y0, x1, y1])\n",
        "\n",
        "        if not words:\n",
        "            words = [\"\"]\n",
        "            boxes = [[0, 0, 0, 0]]\n",
        "\n",
        "        encoding = self.processor(\n",
        "            image, words, boxes=boxes,\n",
        "            return_tensors=\"pt\", truncation=True,\n",
        "            max_length=512, padding=\"max_length\"\n",
        "        )\n",
        "        return encoding\n",
        "\n",
        "    def predict(self, image, ocr_results):\n",
        "        \"\"\"Find vendor/date/total in an image\"\"\"\n",
        "        self.model.eval()\n",
        "        encoding = self.prepare_inputs(image, ocr_results)\n",
        "\n",
        "        for k, v in encoding.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                encoding[k] = v.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**encoding)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        pred_labels = predictions[0].cpu().numpy()\n",
        "        extracted = {'vendor': [], 'date': [], 'total': []}\n",
        "\n",
        "        words = [r['text'].strip() for r in ocr_results if r['text'].strip()]\n",
        "\n",
        "        for i, (word, label_id) in enumerate(zip(words, pred_labels[1:len(words)+1])):\n",
        "            label = self.id2label.get(label_id, 'O')\n",
        "            if 'VENDOR' in label:\n",
        "                extracted['vendor'].append(word)\n",
        "            elif 'DATE' in label:\n",
        "                extracted['date'].append(word)\n",
        "            elif 'TOTAL' in label:\n",
        "                extracted['total'].append(word)\n",
        "\n",
        "        return {\n",
        "            'vendor': ' '.join(extracted['vendor']) if extracted['vendor'] else None,\n",
        "            'date': ' '.join(extracted['date']) if extracted['date'] else None,\n",
        "            'total': ' '.join(extracted['total']) if extracted['total'] else None,\n",
        "        }\n",
        "\n",
        "    def train(self, train_data, epochs=3, lr=5e-5):\n",
        "        \"\"\"Train on labeled receipts\"\"\"\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for image, ocr_results, labels in train_data:\n",
        "                encoding = self.prepare_inputs(image, ocr_results)\n",
        "                for k, v in encoding.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        encoding[k] = v.to(DEVICE)\n",
        "\n",
        "                encoding['labels'] = torch.tensor(labels, device=DEVICE).unsqueeze(0)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(**encoding)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_data):.4f}\")\n",
        "\n",
        "        self.save_model(self.model_path)\n",
        "        return self.model\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(f\"Saved to {path}\")\n",
        "\n",
        "    def load_weights(self, path):\n",
        "        \"\"\"Load model weights\"\"\"\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "        self.model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "        self.model.eval()\n",
        "        print(f\"Loaded from {path}\")\n",
        "\n",
        "# Initialize extractor\n",
        "field_extractor = LayoutLMExtractor(num_labels=NUM_LABELS)\n",
        "\n",
        "SKIP_LAYOUTLM_TRAINING = True\n",
        "\n",
        "if SKIP_LAYOUTLM_TRAINING and os.path.exists(LAYOUTLM_MODEL_PATH):\n",
        "    print(f\"Loading LayoutLM from: {LAYOUTLM_MODEL_PATH}\")\n",
        "    field_extractor.load_weights(LAYOUTLM_MODEL_PATH)\n",
        "else:\n",
        "    print(f\"Initializing LayoutLM, will save to: {LAYOUTLM_MODEL_PATH}\")\n",
        "    field_extractor.load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2341bee3",
      "metadata": {
        "id": "2341bee3"
      },
      "outputs": [],
      "source": [
        "# Train LayoutLMv3 for NER (finding vendor/date/total)\n",
        "\n",
        "import os\n",
        "\n",
        "class ReceiptNERDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for training LayoutLMv3\"\"\"\n",
        "\n",
        "    def __init__(self, receipts, ground_truths, ocr_engine, processor):\n",
        "        self.receipts = receipts\n",
        "        self.ground_truths = ground_truths\n",
        "        self.ocr = ocr_engine\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.receipts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.receipts[idx]\n",
        "        gt = self.ground_truths[idx]\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        ocr_results = self.ocr.extract_with_positions(image)\n",
        "        if not ocr_results:\n",
        "            return None\n",
        "\n",
        "        words = []\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        width, height = image.size\n",
        "\n",
        "        for r in ocr_results:\n",
        "            text = r['text'].strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            bbox = r['bbox']\n",
        "            x0 = int(min(p[0] for p in bbox) * 1000 / width)\n",
        "            y0 = int(min(p[1] for p in bbox) * 1000 / height)\n",
        "            x1 = int(max(p[0] for p in bbox) * 1000 / width)\n",
        "            y1 = int(max(p[1] for p in bbox) * 1000 / height)\n",
        "            x0, y0, x1, y1 = [max(0, min(1000, v)) for v in [x0, y0, x1, y1]]\n",
        "\n",
        "            words.append(text)\n",
        "            boxes.append([x0, y0, x1, y1])\n",
        "\n",
        "            # Assign label based on ground truth\n",
        "            label = 0  # O\n",
        "            text_upper = text.upper()\n",
        "\n",
        "            if gt['vendor'] and text_upper in gt['vendor'].upper():\n",
        "                label = 1  # B-VENDOR\n",
        "            elif gt['date'] and gt['date'] in text:\n",
        "                label = 3  # B-DATE\n",
        "            elif gt['total']:\n",
        "                total_str = f\"{gt['total']:.2f}\"\n",
        "                if total_str in text or text.replace('$', '') == total_str:\n",
        "                    label = 5  # B-TOTAL\n",
        "\n",
        "            labels.append(label)\n",
        "\n",
        "        if not words:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            encoding = self.processor(\n",
        "                image, words, boxes=boxes,\n",
        "                return_tensors=\"pt\", truncation=True,\n",
        "                max_length=512, padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            label_tensor = torch.zeros(512, dtype=torch.long)\n",
        "            label_tensor[:len(labels)] = torch.tensor(labels[:512])\n",
        "\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].squeeze(0),\n",
        "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "                'bbox': encoding['bbox'].squeeze(0),\n",
        "                'pixel_values': encoding['pixel_values'].squeeze(0),\n",
        "                'labels': label_tensor\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Skip None samples and stack the rest\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch:\n",
        "        return None\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'bbox': torch.stack([b['bbox'] for b in batch]),\n",
        "        'pixel_values': torch.stack([b['pixel_values'] for b in batch]),\n",
        "        'labels': torch.stack([b['labels'] for b in batch])\n",
        "    }\n",
        "\n",
        "def train_layoutlm(model, train_loader, epochs=3, lr=5e-5):\n",
        "    \"\"\"Run the training loop\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            bbox = batch['bbox'].to(DEVICE)\n",
        "            pixel_values = batch['pixel_values'].to(DEVICE)\n",
        "            labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                bbox=bbox,\n",
        "                pixel_values=pixel_values,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        if batch_count > 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/batch_count:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Training logic\n",
        "print(\"Training LayoutLMv3...\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "if not SKIP_LAYOUTLM_TRAINING or not os.path.exists(LAYOUTLM_MODEL_PATH):\n",
        "    if torch.cuda.is_available():\n",
        "        train_samples = min(CONFIG['layoutlm_train_samples'], len(synthetic_receipts))\n",
        "        print(f\"Using {train_samples} synthetic receipts for training\")\n",
        "\n",
        "        ner_dataset = ReceiptNERDataset(\n",
        "            synthetic_receipts[:train_samples],\n",
        "            synthetic_ground_truth[:train_samples],\n",
        "            receipt_ocr,\n",
        "            field_extractor.processor\n",
        "        )\n",
        "\n",
        "        ner_loader = DataLoader(ner_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "        try:\n",
        "            train_layoutlm(field_extractor.model, ner_loader, epochs=CONFIG['layoutlm_epochs'])\n",
        "            field_extractor.save_model(LAYOUTLM_MODEL_PATH)\n",
        "            print(\"LayoutLMv3 training complete!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Training failed: {e}\")\n",
        "    else:\n",
        "        print(\"No GPU - skipping LayoutLMv3 training\")\n",
        "        field_extractor.save_model(LAYOUTLM_MODEL_PATH)\n",
        "\n",
        "# Test extractor\n",
        "print(\"Testing LayoutLMv3...\")\n",
        "try:\n",
        "    test_ocr = receipt_ocr.extract_with_positions(synthetic_receipts[0])\n",
        "    test_result = field_extractor.predict(synthetic_receipts[0], test_ocr)\n",
        "    print(f\"Vendor: {test_result['vendor']}\")\n",
        "    print(f\"Date: {test_result['date']}\")\n",
        "    print(f\"Total: {test_result['total']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Test failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987e1689",
      "metadata": {
        "id": "987e1689"
      },
      "outputs": [],
      "source": [
        "# Better field extraction with regex patterns\n",
        "\n",
        "import re\n",
        "\n",
        "class HybridFieldExtractor:\n",
        "    \"\"\"Finds vendor/date/total using regex patterns.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.date_patterns = [\n",
        "            r'\\b(\\d{1,2}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{2,4})\\b',\n",
        "            r'\\b(\\d{4}[/\\-\\.]\\d{1,2}[/\\-\\.]\\d{1,2})\\b',\n",
        "            r'\\b((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s*\\d{1,2},?\\s*\\d{2,4})\\b',\n",
        "        ]\n",
        "        self.time_patterns = [r'\\b(\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?)\\b']\n",
        "\n",
        "        self.final_total_keywords = ['GRAND TOTAL', 'AMOUNT DUE', 'BALANCE DUE', 'TOTAL DUE']\n",
        "        self.total_keywords = ['TOTAL', 'AMOUNT', 'SUM']\n",
        "        self.exclude_keywords = ['SUBTOTAL', 'SUB TOTAL', 'TAX', 'TIP', 'DISCOUNT', 'CHANGE']\n",
        "\n",
        "    def clean_amount_text(self, text):\n",
        "        \"\"\"Fix common OCR mistakes\"\"\"\n",
        "        cleaned = text.strip()\n",
        "        cleaned = re.sub(r'^[Ss](\\d)', r'$\\1', cleaned)\n",
        "        cleaned = re.sub(r'(\\d),(\\d{2})$', r'\\1.\\2', cleaned)\n",
        "        cleaned = re.sub(r'(?<=\\d)[Oo](?=\\d)', '0', cleaned)\n",
        "        return cleaned\n",
        "\n",
        "    def extract_amount(self, text):\n",
        "        \"\"\"Pull a dollar amount from text\"\"\"\n",
        "        cleaned = self.clean_amount_text(text)\n",
        "\n",
        "        patterns = [\n",
        "            (r'\\$\\s*(\\d{1,3}(?:,\\d{3})+\\.\\d{2})', True),\n",
        "            (r'(?<!\\d)(\\d{1,3}(?:,\\d{3})+\\.\\d{2})(?!\\d)', True),\n",
        "            (r'\\$\\s*(\\d{4,}\\.\\d{2})', False),\n",
        "            (r'\\$\\s*(\\d{1,3}\\.\\d{2})', False),\n",
        "            (r'(?<![,\\d])(\\d+\\.\\d{2})(?![,\\d])', False),\n",
        "        ]\n",
        "\n",
        "        for pattern, _ in patterns:\n",
        "            match = re.search(pattern, cleaned, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    amount_str = match.group(1).replace(',', '')\n",
        "                    amount = float(amount_str)\n",
        "                    if amount <= 100000:\n",
        "                        return amount\n",
        "                except ValueError:\n",
        "                    continue\n",
        "        return None\n",
        "\n",
        "    def find_total_amount(self, ocr_results):\n",
        "        \"\"\"Figure out which amount is the actual total\"\"\"\n",
        "        amounts = {'total': None, 'subtotal': None, 'tax': None, 'all_amounts': [], 'method': None}\n",
        "        amount_candidates = []\n",
        "\n",
        "        for idx, r in enumerate(ocr_results):\n",
        "            text = r['text']\n",
        "            text_upper = text.upper()\n",
        "            amount = self.extract_amount(text)\n",
        "\n",
        "            if amount is None and idx + 1 < len(ocr_results):\n",
        "                amount = self.extract_amount(ocr_results[idx + 1]['text'])\n",
        "\n",
        "            if amount is not None and amount > 0:\n",
        "                is_excluded = any(kw in text_upper for kw in self.exclude_keywords)\n",
        "                is_final_total = any(kw in text_upper for kw in self.final_total_keywords)\n",
        "                is_total = any(kw in text_upper for kw in self.total_keywords)\n",
        "                position_score = idx / max(len(ocr_results), 1)\n",
        "\n",
        "                amount_candidates.append({\n",
        "                    'amount': amount, 'text': text, 'position': idx,\n",
        "                    'position_score': position_score, 'is_final_total': is_final_total,\n",
        "                    'is_total': is_total, 'is_excluded': is_excluded\n",
        "                })\n",
        "                amounts['all_amounts'].append(amount)\n",
        "\n",
        "                if 'SUBTOTAL' in text_upper:\n",
        "                    amounts['subtotal'] = amount\n",
        "                elif 'TAX' in text_upper:\n",
        "                    amounts['tax'] = amount\n",
        "\n",
        "        # Priority selection\n",
        "        final_candidates = [c for c in amount_candidates if c['is_final_total']]\n",
        "        if final_candidates:\n",
        "            amounts['total'] = max(final_candidates, key=lambda x: x['position_score'])['amount']\n",
        "            amounts['method'] = 'final_keyword'\n",
        "            return amounts\n",
        "\n",
        "        total_candidates = [c for c in amount_candidates if c['is_total'] and not c['is_excluded']]\n",
        "        if total_candidates:\n",
        "            amounts['total'] = max(total_candidates, key=lambda x: x['position_score'])['amount']\n",
        "            amounts['method'] = 'total_keyword'\n",
        "            return amounts\n",
        "\n",
        "        bottom_half = [c for c in amount_candidates if c['position_score'] > 0.5 and not c['is_excluded']]\n",
        "        if bottom_half:\n",
        "            amounts['total'] = max(bottom_half, key=lambda x: x['amount'])['amount']\n",
        "            amounts['method'] = 'bottom_largest'\n",
        "            return amounts\n",
        "\n",
        "        if amount_candidates:\n",
        "            amounts['total'] = max(amount_candidates, key=lambda x: x['amount'])['amount']\n",
        "            amounts['method'] = 'fallback_largest'\n",
        "\n",
        "        return amounts\n",
        "\n",
        "    def extract(self, ocr_results, image=None):\n",
        "        \"\"\"Get all the fields from OCR results\"\"\"\n",
        "        if not ocr_results:\n",
        "            return {'vendor': None, 'date': None, 'time': None, 'total': None,\n",
        "                    'subtotal': None, 'tax': None, 'items': [], 'raw_text': ''}\n",
        "\n",
        "        all_text = '\\n'.join([r['text'] for r in ocr_results])\n",
        "        result = {'vendor': None, 'date': None, 'time': None, 'total': None,\n",
        "                  'subtotal': None, 'tax': None, 'items': [], 'raw_text': all_text}\n",
        "\n",
        "        # Vendor (first non-numeric line)\n",
        "        for r in ocr_results[:5]:\n",
        "            line = r['text'].strip()\n",
        "            if not re.match(r'^[\\d\\s\\-\\/\\.\\:\\$\\,]+$', line) and len(line) > 2:\n",
        "                result['vendor'] = line\n",
        "                break\n",
        "\n",
        "        # Date\n",
        "        for pattern in self.date_patterns:\n",
        "            match = re.search(pattern, all_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                result['date'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Time\n",
        "        for pattern in self.time_patterns:\n",
        "            match = re.search(pattern, all_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                result['time'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Amounts\n",
        "        amounts = self.find_total_amount(ocr_results)\n",
        "        result['total'] = amounts['total']\n",
        "        result['subtotal'] = amounts['subtotal']\n",
        "        result['tax'] = amounts['tax']\n",
        "\n",
        "        return result\n",
        "\n",
        "    def predict(self, image, ocr_results):\n",
        "        \"\"\"Alias for extract()\"\"\"\n",
        "        return self.extract(ocr_results, image)\n",
        "\n",
        "# Initialize\n",
        "hybrid_extractor = HybridFieldExtractor()\n",
        "\n",
        "# Test\n",
        "print(\"Testing amount extraction...\")\n",
        "test_cases = [\"$11,812.50\", \"$1,234.56\", \"$812.50\", \"TOTAL: $99.99\"]\n",
        "for test in test_cases:\n",
        "    result = hybrid_extractor.extract_amount(test)\n",
        "    print(f\"  '{test}' -> ${result:.2f}\" if result else f\"  '{test}' -> None\")\n",
        "\n",
        "# Test on synthetic receipts\n",
        "print(\"\\nTesting on synthetic receipts...\")\n",
        "correct = 0\n",
        "for i in range(min(5, len(synthetic_receipts))):\n",
        "    test_ocr = receipt_ocr.extract_with_positions(synthetic_receipts[i])\n",
        "    extracted = hybrid_extractor.extract(test_ocr)\n",
        "    gt = synthetic_ground_truth[i]\n",
        "    if extracted['total'] and abs(extracted['total'] - gt['total']) < 0.01:\n",
        "        correct += 1\n",
        "print(f\"Total accuracy: {correct}/5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e91379",
      "metadata": {
        "id": "08e91379"
      },
      "source": [
        "## Anomaly Detection\n",
        "Catch weird receipts (crazy amounts, missing fields, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cb5cd3",
      "metadata": {
        "id": "10cb5cd3"
      },
      "outputs": [],
      "source": [
        "# Anomaly detector\n",
        "\n",
        "ANOMALY_MODEL_PATH = os.path.join(MODELS_DIR, 'anomaly_detector.pt')\n",
        "\n",
        "class ReceiptAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Uses Isolation Forest to flag weird receipts.\n",
        "    Stuff like $50k totals or missing vendors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, contamination=0.1):\n",
        "        self.contamination = contamination\n",
        "        self.model = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=contamination,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.is_fitted = False\n",
        "        self.feature_names = ['amount', 'vendor_len', 'date_valid', 'num_items', 'hour']\n",
        "        self.model_path = ANOMALY_MODEL_PATH\n",
        "\n",
        "    def extract_features(self, receipt_data: dict) -> np.ndarray:\n",
        "        \"\"\"Turn receipt data into numbers for the model\"\"\"\n",
        "        import re\n",
        "        from datetime import datetime\n",
        "\n",
        "        # Amount feature\n",
        "        amount = receipt_data.get('total', 0)\n",
        "        if isinstance(amount, str):\n",
        "            amount = float(re.sub(r'[^\\d.]', '', amount) or 0)\n",
        "\n",
        "        # Vendor length (proxy for validity)\n",
        "        vendor = receipt_data.get('vendor', '') or ''\n",
        "        vendor_len = len(vendor)\n",
        "\n",
        "        # Date validity (1 if valid date, 0 otherwise)\n",
        "        date_str = receipt_data.get('date', '')\n",
        "        date_valid = 0\n",
        "        if date_str:\n",
        "            for fmt in ['%m/%d/%Y', '%m/%d/%y', '%Y-%m-%d', '%d-%m-%Y']:\n",
        "                try:\n",
        "                    parsed = datetime.strptime(date_str, fmt)\n",
        "                    date_valid = 1\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # Number of items (if available)\n",
        "        num_items = len(receipt_data.get('items', [])) if 'items' in receipt_data else 3\n",
        "\n",
        "        # Hour of transaction (if time available)\n",
        "        time_str = receipt_data.get('time', '')\n",
        "        hour = 12  # Default\n",
        "        if time_str:\n",
        "            try:\n",
        "                hour = int(time_str.split(':')[0])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return np.array([[amount, vendor_len, date_valid, num_items, hour]])\n",
        "\n",
        "    def fit(self, receipt_data_list: list):\n",
        "        \"\"\"Train on a bunch of receipts\"\"\"\n",
        "        features = []\n",
        "        for data in receipt_data_list:\n",
        "            feat = self.extract_features(data)\n",
        "            features.append(feat[0])\n",
        "\n",
        "        X = np.array(features)\n",
        "\n",
        "        # Handle edge cases\n",
        "        if len(X) < 10:\n",
        "            print(\"Not enough samples for anomaly detection\")\n",
        "            synthetic_normal = np.random.normal(\n",
        "                loc=X.mean(axis=0) if len(X) > 0 else [50, 10, 1, 5, 14],\n",
        "                scale=X.std(axis=0) if len(X) > 0 else [20, 5, 0.1, 2, 3],\n",
        "                size=(100, 5)\n",
        "            )\n",
        "            X = np.vstack([X, synthetic_normal]) if len(X) > 0 else synthetic_normal\n",
        "\n",
        "        self.model.fit(X)\n",
        "        self.is_fitted = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, receipt_data: dict) -> dict:\n",
        "        \"\"\"Check if a receipt looks suspicious\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
        "\n",
        "        features = self.extract_features(receipt_data)\n",
        "\n",
        "        # Get anomaly score (-1 for anomaly, 1 for normal)\n",
        "        prediction = self.model.predict(features)[0]\n",
        "        score = self.model.decision_function(features)[0]\n",
        "\n",
        "        # Identify reasons for anomaly\n",
        "        reasons = []\n",
        "        amount = features[0][0]\n",
        "        vendor_len = features[0][1]\n",
        "        date_valid = features[0][2]\n",
        "\n",
        "        if amount > 1000:\n",
        "            reasons.append(f\"High amount: ${amount:.2f}\")\n",
        "        elif amount < 1:\n",
        "            reasons.append(f\"Suspiciously low amount: ${amount:.2f}\")\n",
        "\n",
        "        if vendor_len < 2:\n",
        "            reasons.append(\"Missing or invalid vendor\")\n",
        "\n",
        "        if date_valid == 0:\n",
        "            reasons.append(\"Invalid or missing date\")\n",
        "\n",
        "        return {\n",
        "            'is_anomaly': prediction == -1,\n",
        "            'score': float(score),\n",
        "            'prediction': 'ANOMALY' if prediction == -1 else 'NORMAL',\n",
        "            'reasons': reasons,\n",
        "            'features': dict(zip(self.feature_names, features[0]))\n",
        "        }\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"Save the trained model\"\"\"\n",
        "        model_data = {\n",
        "            'model': self.model,\n",
        "            'is_fitted': self.is_fitted,\n",
        "            'contamination': self.contamination,\n",
        "            'feature_names': self.feature_names\n",
        "        }\n",
        "        torch.save(model_data, path)\n",
        "        print(f\"Anomaly detector saved to: {path}\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"Load a trained model\"\"\"\n",
        "        # weights_only=False needed for sklearn models\n",
        "        model_data = torch.load(path, map_location='cpu', weights_only=False)\n",
        "        self.model = model_data['model']\n",
        "        self.is_fitted = model_data['is_fitted']\n",
        "        self.contamination = model_data['contamination']\n",
        "        self.feature_names = model_data['feature_names']\n",
        "        print(f\"Anomaly detector loaded from: {path}\")\n",
        "\n",
        "\n",
        "# Initialize and train anomaly detector\n",
        "SKIP_ANOMALY_TRAINING = True  # Set to False to force retraining\n",
        "\n",
        "anomaly_detector = ReceiptAnomalyDetector(contamination=0.1)\n",
        "\n",
        "if SKIP_ANOMALY_TRAINING and os.path.exists(ANOMALY_MODEL_PATH):\n",
        "    print(f\"Loading anomaly detector from: {ANOMALY_MODEL_PATH}\")\n",
        "    anomaly_detector.load_model(ANOMALY_MODEL_PATH)\n",
        "else:\n",
        "    print(f\"Training anomaly detector, will save to: {ANOMALY_MODEL_PATH}\")\n",
        "    # Create training data from synthetic receipts\n",
        "    training_data = []\n",
        "    for gt in synthetic_ground_truth:\n",
        "        training_data.append({\n",
        "            'vendor': gt['vendor'],\n",
        "            'date': gt['date'],\n",
        "            'time': gt['time'],\n",
        "            'total': gt['total'],\n",
        "            'items': gt['items']\n",
        "        })\n",
        "\n",
        "    # Add some anomalous samples for training\n",
        "    anomalous_samples = [\n",
        "        {'vendor': '', 'date': 'invalid', 'total': 50000, 'time': '25:00'},\n",
        "        {'vendor': 'X', 'date': '', 'total': 0.01, 'time': ''},\n",
        "        {'vendor': 'SUSPICIOUS VENDOR', 'date': '99/99/9999', 'total': -100, 'time': ''},\n",
        "    ]\n",
        "    training_data.extend(anomalous_samples)\n",
        "\n",
        "    # Fit the model\n",
        "    anomaly_detector.fit(training_data)\n",
        "\n",
        "    # Save model\n",
        "    anomaly_detector.save_model(ANOMALY_MODEL_PATH)\n",
        "\n",
        "# Test on normal and anomalous receipts\n",
        "print(\"Testing anomaly detection...\")\n",
        "\n",
        "test_normal = {\n",
        "    'vendor': synthetic_ground_truth[0]['vendor'],\n",
        "    'date': synthetic_ground_truth[0]['date'],\n",
        "    'time': synthetic_ground_truth[0]['time'],\n",
        "    'total': synthetic_ground_truth[0]['total'],\n",
        "    'items': synthetic_ground_truth[0]['items']\n",
        "}\n",
        "\n",
        "normal_result = anomaly_detector.predict(test_normal)\n",
        "print(f\"Normal receipt: {normal_result['prediction']}\")\n",
        "\n",
        "test_anomalous = {'vendor': '', 'date': 'invalid', 'total': 50000, 'time': '25:00'}\n",
        "anomaly_result = anomaly_detector.predict(test_anomalous)\n",
        "print(f\"Anomalous receipt: {anomaly_result['prediction']}\")\n",
        "if anomaly_result['reasons']:\n",
        "    print(f\"Reasons: {anomaly_result['reasons']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03df96db",
      "metadata": {
        "id": "03df96db"
      },
      "source": [
        "## LangGraph Tools\n",
        "Define the functions our agent workflow will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83231fc6",
      "metadata": {
        "id": "83231fc6"
      },
      "outputs": [],
      "source": [
        "# Define our agent tools\n",
        "\n",
        "from typing import Annotated\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# What our state looks like as it goes through the pipeline\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"Holds all the data as we process a receipt\"\"\"\n",
        "    image: Optional[Image.Image]\n",
        "    image_path: Optional[str]\n",
        "    ocr_results: Optional[list]\n",
        "    ocr_text: Optional[str]\n",
        "    classification: Optional[dict]\n",
        "    extracted_fields: Optional[dict]\n",
        "    anomaly_result: Optional[dict]\n",
        "    decision: Optional[str]\n",
        "    confidence_score: Optional[float]\n",
        "    processing_log: list\n",
        "    error: Optional[str]\n",
        "\n",
        "\n",
        "@tool\n",
        "def classify_document(image: Image.Image) -> dict:\n",
        "    \"\"\"Check if image is a receipt or something else\"\"\"\n",
        "    try:\n",
        "        result = doc_classifier.predict(image)\n",
        "        return {\n",
        "            'success': True,\n",
        "            'is_receipt': result['is_receipt'],\n",
        "            'confidence': result['confidence'],\n",
        "            'label': result['label']\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'success': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "@tool\n",
        "def extract_text_ocr(image: Image.Image) -> dict:\n",
        "    \"\"\"Run OCR on the image\"\"\"\n",
        "    try:\n",
        "        ocr_results = receipt_ocr.extract_with_positions(image)\n",
        "        processed = receipt_ocr.postprocess_receipt(ocr_results)\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'num_regions': len(ocr_results),\n",
        "            'ocr_results': ocr_results,\n",
        "            'processed': processed,\n",
        "            'raw_text': processed.get('raw_text', '')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'success': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "@tool\n",
        "def extract_receipt_fields(image: Image.Image, ocr_results: list) -> dict:\n",
        "    \"\"\"Find vendor, date, total in the receipt\"\"\"\n",
        "    try:\n",
        "        # Use LayoutLM for field extraction\n",
        "        layoutlm_result = field_extractor.predict(image, ocr_results)\n",
        "\n",
        "        # Also get post-processed OCR fields as fallback\n",
        "        ocr_fields = receipt_ocr.postprocess_receipt(ocr_results)\n",
        "\n",
        "        # Merge results (prefer LayoutLM, fallback to OCR)\n",
        "        fields = {\n",
        "            'vendor': layoutlm_result.get('vendor') or ocr_fields.get('vendor'),\n",
        "            'date': layoutlm_result.get('date') or ocr_fields.get('date'),\n",
        "            'total': layoutlm_result.get('total') or ocr_fields.get('total'),\n",
        "            'time': ocr_fields.get('time'),\n",
        "            'all_amounts': ocr_fields.get('all_amounts', []),\n",
        "            'extraction_source': 'layoutlm+ocr'\n",
        "        }\n",
        "\n",
        "        return {'success': True, 'fields': fields}\n",
        "    except Exception as e:\n",
        "        return {'success': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "@tool\n",
        "def detect_anomalies(extracted_fields: dict) -> dict:\n",
        "    \"\"\"Check if anything looks fishy\"\"\"\n",
        "    try:\n",
        "        result = anomaly_detector.predict(extracted_fields)\n",
        "        return {\n",
        "            'success': True,\n",
        "            'is_anomaly': result['is_anomaly'],\n",
        "            'score': result['score'],\n",
        "            'prediction': result['prediction'],\n",
        "            'reasons': result['reasons']\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {'success': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "@tool\n",
        "def make_routing_decision(\n",
        "    classification: dict,\n",
        "    anomaly_result: dict,\n",
        "    extracted_fields: dict\n",
        ") -> dict:\n",
        "    \"\"\"Decide if we should approve, review, or reject\"\"\"\n",
        "    decision = \"REVIEW\"  # Default to human review\n",
        "    reasons = []\n",
        "    confidence = 0.5\n",
        "\n",
        "    # Check classification confidence\n",
        "    class_conf = classification.get('confidence', 0)\n",
        "    if class_conf < 0.7:\n",
        "        reasons.append(f\"Low document confidence: {class_conf:.2%}\")\n",
        "    elif class_conf > 0.9:\n",
        "        confidence += 0.2\n",
        "\n",
        "    # Check if it's actually a receipt\n",
        "    if not classification.get('is_receipt', False):\n",
        "        decision = \"REJECT\"\n",
        "        reasons.append(\"Not classified as receipt/invoice\")\n",
        "        confidence = class_conf\n",
        "        return {\n",
        "            'decision': decision,\n",
        "            'confidence': confidence,\n",
        "            'reasons': reasons\n",
        "        }\n",
        "\n",
        "    # Check anomaly status\n",
        "    if anomaly_result.get('is_anomaly', False):\n",
        "        decision = \"REVIEW\"\n",
        "        reasons.extend(anomaly_result.get('reasons', ['Anomaly detected']))\n",
        "        confidence = max(0.3, confidence - 0.2)\n",
        "    else:\n",
        "        confidence += 0.2\n",
        "\n",
        "    # Check extracted fields completeness\n",
        "    fields = extracted_fields.get('fields', {})\n",
        "    missing_fields = []\n",
        "    for field in ['vendor', 'date', 'total']:\n",
        "        if not fields.get(field):\n",
        "            missing_fields.append(field)\n",
        "\n",
        "    if missing_fields:\n",
        "        reasons.append(f\"Missing fields: {', '.join(missing_fields)}\")\n",
        "        confidence -= 0.1 * len(missing_fields)\n",
        "    else:\n",
        "        confidence += 0.1\n",
        "\n",
        "    # Final decision logic\n",
        "    confidence = min(1.0, max(0.0, confidence))\n",
        "\n",
        "    if confidence > 0.85 and not anomaly_result.get('is_anomaly', False):\n",
        "        decision = \"APPROVE\"\n",
        "    elif confidence < 0.4 or anomaly_result.get('is_anomaly', False):\n",
        "        decision = \"REVIEW\"\n",
        "    else:\n",
        "        decision = \"APPROVE\"\n",
        "\n",
        "    return {\n",
        "        'decision': decision,\n",
        "        'confidence': confidence,\n",
        "        'reasons': reasons if reasons else ['All checks passed']\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe0547f",
      "metadata": {
        "id": "bfe0547f"
      },
      "source": [
        "## LangGraph Workflow\n",
        "Wire up all the pieces into a pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d919523",
      "metadata": {
        "id": "0d919523"
      },
      "outputs": [],
      "source": [
        "# Build the workflow\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def ingestion_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Load and prep the image\"\"\"\n",
        "    state['processing_log'] = state.get('processing_log', [])\n",
        "    state['processing_log'].append(\"Ingestion: Starting receipt processing\")\n",
        "\n",
        "    try:\n",
        "        image = state.get('image')\n",
        "        image_path = state.get('image_path')\n",
        "\n",
        "        if image is None and image_path:\n",
        "            image = Image.open(image_path)\n",
        "            state['image'] = image\n",
        "\n",
        "        if image is None:\n",
        "            state['error'] = \"No image provided\"\n",
        "            return state\n",
        "\n",
        "        # Convert to RGB if needed\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "            state['image'] = image\n",
        "\n",
        "        state['processing_log'].append(f\"Image loaded: {image.size}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"Ingestion error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def classification_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Run the classifier\"\"\"\n",
        "    state['processing_log'].append(\"Classification: Analyzing document type\")\n",
        "\n",
        "    try:\n",
        "        image = state.get('image')\n",
        "        if image is None:\n",
        "            state['error'] = \"No image available for classification\"\n",
        "            return state\n",
        "\n",
        "        result = doc_classifier.predict(image)\n",
        "        state['classification'] = result\n",
        "\n",
        "        label = result['label']\n",
        "        conf = result['confidence']\n",
        "        state['processing_log'].append(f\"Result: {label} ({conf:.2%} confidence)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"Classification error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "        state['classification'] = {'is_receipt': False, 'confidence': 0, 'label': 'error'}\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def ocr_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Extract text using OCR\"\"\"\n",
        "    state['processing_log'].append(\"OCR: Extracting text from image\")\n",
        "\n",
        "    try:\n",
        "        image = state.get('image')\n",
        "        if image is None:\n",
        "            state['error'] = \"No image available for OCR\"\n",
        "            return state\n",
        "\n",
        "        ocr_results = receipt_ocr.extract_with_positions(image)\n",
        "        processed = receipt_ocr.postprocess_receipt(ocr_results)\n",
        "\n",
        "        state['ocr_results'] = ocr_results\n",
        "        state['ocr_text'] = processed.get('raw_text', '')\n",
        "\n",
        "        state['processing_log'].append(f\"Extracted {len(ocr_results)} text regions\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"OCR error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "        state['ocr_results'] = []\n",
        "        state['ocr_text'] = ''\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def extraction_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Extract structured fields using LayoutLM\"\"\"\n",
        "    state['processing_log'].append(\"Extraction: Identifying receipt fields\")\n",
        "\n",
        "    try:\n",
        "        image = state.get('image')\n",
        "        ocr_results = state.get('ocr_results', [])\n",
        "\n",
        "        if image is None or not ocr_results:\n",
        "            fields = receipt_ocr.postprocess_receipt(ocr_results) if ocr_results else {}\n",
        "            state['extracted_fields'] = fields\n",
        "            state['processing_log'].append(\"Using OCR-only extraction\")\n",
        "            return state\n",
        "\n",
        "        # Use LayoutLM for extraction\n",
        "        layoutlm_fields = field_extractor.predict(image, ocr_results)\n",
        "        ocr_fields = receipt_ocr.postprocess_receipt(ocr_results)\n",
        "\n",
        "        # Merge results\n",
        "        fields = {\n",
        "            'vendor': layoutlm_fields.get('vendor') or ocr_fields.get('vendor'),\n",
        "            'date': layoutlm_fields.get('date') or ocr_fields.get('date'),\n",
        "            'total': layoutlm_fields.get('total') or ocr_fields.get('total'),\n",
        "            'time': ocr_fields.get('time'),\n",
        "            'all_amounts': ocr_fields.get('all_amounts', [])\n",
        "        }\n",
        "\n",
        "        state['extracted_fields'] = fields\n",
        "        state['processing_log'].append(f\"Extracted: vendor={fields.get('vendor')}, total=${fields.get('total')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"Extraction error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "        state['extracted_fields'] = {}\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def anomaly_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Check for suspicious patterns\"\"\"\n",
        "    state['processing_log'].append(\"Anomaly Detection: Checking for suspicious patterns\")\n",
        "\n",
        "    try:\n",
        "        extracted = state.get('extracted_fields', {})\n",
        "\n",
        "        if not extracted:\n",
        "            state['anomaly_result'] = {\n",
        "                'is_anomaly': True,\n",
        "                'score': -1.0,\n",
        "                'prediction': 'ANOMALY',\n",
        "                'reasons': ['No data extracted']\n",
        "            }\n",
        "            state['processing_log'].append(\"No data to analyze\")\n",
        "            return state\n",
        "\n",
        "        result = anomaly_detector.predict(extracted)\n",
        "        state['anomaly_result'] = result\n",
        "\n",
        "        status = \"ANOMALY\" if result['is_anomaly'] else \"NORMAL\"\n",
        "        state['processing_log'].append(f\"{status} (score: {result['score']:.3f})\")\n",
        "\n",
        "        if result['reasons']:\n",
        "            for reason in result['reasons']:\n",
        "                state['processing_log'].append(f\"  - {reason}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"Anomaly detection error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "        state['anomaly_result'] = {'is_anomaly': False, 'score': 0, 'reasons': []}\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def routing_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"Make final decision based on all results\"\"\"\n",
        "    state['processing_log'].append(\"Routing: Making final decision\")\n",
        "\n",
        "    try:\n",
        "        classification = state.get('classification', {})\n",
        "        anomaly_result = state.get('anomaly_result', {})\n",
        "        extracted_fields = state.get('extracted_fields', {})\n",
        "\n",
        "        # Decision logic\n",
        "        is_receipt = classification.get('is_receipt', False)\n",
        "        class_conf = classification.get('confidence', 0)\n",
        "        is_anomaly = anomaly_result.get('is_anomaly', False)\n",
        "        anomaly_score = anomaly_result.get('score', 0)\n",
        "\n",
        "        # Calculate overall confidence\n",
        "        confidence = class_conf\n",
        "\n",
        "        # Determine decision\n",
        "        if not is_receipt:\n",
        "            decision = \"REJECT\"\n",
        "            confidence = class_conf\n",
        "            reason = \"Not a receipt/invoice\"\n",
        "        elif is_anomaly:\n",
        "            decision = \"REVIEW\"\n",
        "            confidence = max(0.3, confidence - 0.2)\n",
        "            reason = \"Anomaly detected - requires human review\"\n",
        "        elif class_conf > 0.9 and anomaly_score > 0:\n",
        "            decision = \"APPROVE\"\n",
        "            confidence = min(0.95, confidence + 0.1)\n",
        "            reason = \"High confidence, no anomalies\"\n",
        "        elif class_conf > 0.7:\n",
        "            decision = \"APPROVE\"\n",
        "            reason = \"Acceptable confidence\"\n",
        "        else:\n",
        "            decision = \"REVIEW\"\n",
        "            reason = \"Low confidence - requires review\"\n",
        "\n",
        "        state['decision'] = decision\n",
        "        state['confidence_score'] = confidence\n",
        "\n",
        "        state['processing_log'].append(f\"Decision: {decision}\")\n",
        "        state['processing_log'].append(f\"Confidence: {confidence:.2%}\")\n",
        "        state['processing_log'].append(f\"Reason: {reason}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        state['error'] = f\"Routing error: {str(e)}\"\n",
        "        state['processing_log'].append(f\"Error: {str(e)}\")\n",
        "        state['decision'] = \"REVIEW\"\n",
        "        state['confidence_score'] = 0.0\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def should_continue(state: AgentState) -> Literal[\"continue\", \"end\"]:\n",
        "    \"\"\"Determine if workflow should continue or end early\"\"\"\n",
        "    if state.get('error'):\n",
        "        return \"end\"\n",
        "    if state.get('classification', {}).get('is_receipt', True) == False:\n",
        "        return \"end\"\n",
        "    return \"continue\"\n",
        "\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"ingest\", ingestion_node)\n",
        "workflow.add_node(\"classify\", classification_node)\n",
        "workflow.add_node(\"ocr\", ocr_node)\n",
        "workflow.add_node(\"extract\", extraction_node)\n",
        "workflow.add_node(\"anomaly\", anomaly_node)\n",
        "workflow.add_node(\"route\", routing_node)\n",
        "\n",
        "# Define edges (sequential flow)\n",
        "workflow.set_entry_point(\"ingest\")\n",
        "workflow.add_edge(\"ingest\", \"classify\")\n",
        "workflow.add_edge(\"classify\", \"ocr\")\n",
        "workflow.add_edge(\"ocr\", \"extract\")\n",
        "workflow.add_edge(\"extract\", \"anomaly\")\n",
        "workflow.add_edge(\"anomaly\", \"route\")\n",
        "workflow.add_edge(\"route\", END)\n",
        "\n",
        "# Compile the workflow\n",
        "receipt_agent = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23219221",
      "metadata": {
        "id": "23219221"
      },
      "outputs": [],
      "source": [
        "# Test it out on a fake receipt\n",
        "\n",
        "test_image = synthetic_receipts[0]\n",
        "test_gt = synthetic_ground_truth[0]\n",
        "\n",
        "# Initialize state\n",
        "initial_state = {\n",
        "    'image': test_image,\n",
        "    'image_path': None,\n",
        "    'ocr_results': None,\n",
        "    'ocr_text': None,\n",
        "    'classification': None,\n",
        "    'extracted_fields': None,\n",
        "    'anomaly_result': None,\n",
        "    'decision': None,\n",
        "    'confidence_score': None,\n",
        "    'processing_log': [],\n",
        "    'error': None\n",
        "}\n",
        "\n",
        "# Run the workflow\n",
        "result = receipt_agent.invoke(initial_state)\n",
        "\n",
        "# Display results\n",
        "for log in result['processing_log']:\n",
        "    print(log)\n",
        "\n",
        "if result.get('error'):\n",
        "    print(f\"Error: {result['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c5f534",
      "metadata": {
        "id": "42c5f534"
      },
      "source": [
        "## Demo UI\n",
        "Gradio interface so you can actually try this thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672c345e",
      "metadata": {
        "id": "672c345e"
      },
      "outputs": [],
      "source": [
        "# Gradio demo - works in Colab!\n",
        "\n",
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re\n",
        "\n",
        "\n",
        "def process_receipt(image):\n",
        "    \"\"\"Main function - takes an image, returns all the extracted info\"\"\"\n",
        "\n",
        "    # Return 10 empty values if no image\n",
        "    if image is None:\n",
        "        return (\"Please upload an image\", \"\", \"\", \"\", \"\", \"\",\n",
        "                \"No image\", \"No image\", \"Please upload an image to process\", \"\")\n",
        "\n",
        "    # Convert to PIL if numpy array\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    processing_log = []\n",
        "\n",
        "    # Step 1: Classification\n",
        "    processing_log.append(\"Step 1: Classifying document...\")\n",
        "    try:\n",
        "        inputs = vit_processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = doc_classifier.model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        receipt_prob = probs[0][1].item()\n",
        "        is_receipt = receipt_prob > 0.5\n",
        "        doc_type = \"RECEIPT\" if is_receipt else \"OTHER DOCUMENT\"\n",
        "        confidence = f\"{receipt_prob:.1%}\"\n",
        "        processing_log.append(f\"Classification: {doc_type} ({confidence})\")\n",
        "    except Exception as e:\n",
        "        processing_log.append(f\"Classification error: {str(e)}\")\n",
        "        doc_type = \"Unknown\"\n",
        "        confidence = \"0%\"\n",
        "        is_receipt = False\n",
        "\n",
        "    # Step 2: OCR\n",
        "    processing_log.append(\"Step 2: Extracting text with OCR...\")\n",
        "    ocr_results = []\n",
        "    full_text = \"\"\n",
        "    try:\n",
        "        img_array = np.array(image)\n",
        "        ocr_raw = receipt_ocr.reader.readtext(img_array, detail=1)\n",
        "        ocr_results = [{'text': text, 'confidence': conf, 'bbox': bbox} for bbox, text, conf in ocr_raw]\n",
        "        full_text = ' '.join([r['text'] for r in ocr_results])\n",
        "        processing_log.append(f\"OCR: Found {len(ocr_results)} text regions\")\n",
        "    except Exception as e:\n",
        "        processing_log.append(f\"OCR error: {str(e)}\")\n",
        "\n",
        "    # Step 3: Field Extraction\n",
        "    processing_log.append(\"Step 3: Extracting fields...\")\n",
        "    extracted = {}\n",
        "    vendor = \"Not detected\"\n",
        "    date = \"Not detected\"\n",
        "    total = \"$0.00\"\n",
        "    amount_breakdown = \"\"\n",
        "\n",
        "    try:\n",
        "        extracted = hybrid_extractor.extract(ocr_results, image)\n",
        "\n",
        "        vendor = extracted.get('vendor') or \"Not detected\"\n",
        "        date = extracted.get('date') or \"Not detected\"\n",
        "\n",
        "        total_val = extracted.get('total')\n",
        "        if total_val is not None:\n",
        "            total = f\"${float(total_val):.2f}\"\n",
        "        else:\n",
        "            total = \"$0.00\"\n",
        "\n",
        "        # Build amount breakdown\n",
        "        breakdown_parts = []\n",
        "        if extracted.get('subtotal'):\n",
        "            breakdown_parts.append(f\"Subtotal: ${extracted['subtotal']:.2f}\")\n",
        "        if extracted.get('discount'):\n",
        "            breakdown_parts.append(f\"Discount: -${extracted['discount']:.2f}\")\n",
        "        if extracted.get('tax'):\n",
        "            breakdown_parts.append(f\"Tax: ${extracted['tax']:.2f}\")\n",
        "        if total_val:\n",
        "            breakdown_parts.append(f\"TOTAL: ${total_val:.2f}\")\n",
        "\n",
        "        method = extracted.get('extraction_method', 'unknown')\n",
        "        breakdown_parts.append(f\"\\n[Method: {method}]\")\n",
        "\n",
        "        amount_breakdown = \"\\n\".join(breakdown_parts) if breakdown_parts else \"No breakdown available\"\n",
        "        processing_log.append(f\"Vendor: {vendor}\")\n",
        "        processing_log.append(f\"Date: {date}\")\n",
        "        processing_log.append(f\"Total: {total} (method: {method})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        processing_log.append(f\"Extraction error: {str(e)}\")\n",
        "        import traceback\n",
        "        processing_log.append(f\"{traceback.format_exc()[:200]}\")\n",
        "        amount_breakdown = f\"Error: {str(e)}\"\n",
        "\n",
        "    # Step 4: Anomaly Detection\n",
        "    processing_log.append(\"Step 4: Checking for anomalies...\")\n",
        "    is_anomaly = False\n",
        "    anomaly_status = \"NORMAL\"\n",
        "    try:\n",
        "        total_numeric = extracted.get('total', 0) or 0\n",
        "\n",
        "        anomaly_result = anomaly_detector.predict({\n",
        "            'total': total_numeric,\n",
        "            'vendor': vendor if vendor != 'Not detected' else '',\n",
        "            'date': date if date != 'Not detected' else None\n",
        "        })\n",
        "        is_anomaly = anomaly_result.get('is_anomaly', False)\n",
        "        anomaly_status = \"ANOMALY DETECTED\" if is_anomaly else \"NORMAL\"\n",
        "        processing_log.append(f\"Anomaly Check: {anomaly_status}\")\n",
        "    except Exception as e:\n",
        "        processing_log.append(f\"Anomaly detection error: {str(e)}\")\n",
        "\n",
        "    # Step 5: Final Decision\n",
        "    processing_log.append(\"Step 5: Making final decision...\")\n",
        "    try:\n",
        "        conf_value = float(confidence.replace('%', '')) / 100\n",
        "    except:\n",
        "        conf_value = 0\n",
        "\n",
        "    if not is_receipt:\n",
        "        decision = \"REJECT - Not a receipt\"\n",
        "    elif is_anomaly:\n",
        "        decision = \"REVIEW - Anomaly detected\"\n",
        "    elif conf_value > 0.7:\n",
        "        decision = \"APPROVE - Valid receipt\"\n",
        "    else:\n",
        "        decision = \"REVIEW - Low confidence\"\n",
        "\n",
        "    processing_log.append(f\"Final Decision: {decision}\")\n",
        "\n",
        "    log_text = \"\\n\".join(processing_log)\n",
        "    ocr_preview = full_text\n",
        "\n",
        "    return doc_type, confidence, vendor, date, total, amount_breakdown, decision, anomaly_status, log_text, ocr_preview\n",
        "\n",
        "\n",
        "# Build the UI\n",
        "with gr.Blocks(title=\"Receipt Automation Agent\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Receipt Automation Agent V2\n",
        "\n",
        "    Upload a receipt image to automatically:\n",
        "    - **Classify** if it's a valid receipt\n",
        "    - **Extract** vendor, date, and total amount\n",
        "    - **Break down** subtotal, tax, discounts vs final total\n",
        "    - **Detect anomalies** in the receipt data\n",
        "    - **Make a decision** (Approve / Review / Reject)\n",
        "\n",
        "    ---\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Upload Receipt\")\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Receipt Image\")\n",
        "            process_btn = gr.Button(\"Process Receipt\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            gr.Markdown(\"### Extracted Text (OCR)\")\n",
        "            ocr_output = gr.Textbox(label=\"OCR Full Text\", lines=15, interactive=False, max_lines=30)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Results\")\n",
        "\n",
        "            with gr.Row():\n",
        "                doc_type_output = gr.Textbox(label=\"Document Type\", interactive=False)\n",
        "                confidence_output = gr.Textbox(label=\"Confidence\", interactive=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                vendor_output = gr.Textbox(label=\"Vendor\", interactive=False)\n",
        "                date_output = gr.Textbox(label=\"Date\", interactive=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                total_output = gr.Textbox(label=\"Final Total\", interactive=False)\n",
        "                anomaly_output = gr.Textbox(label=\"Anomaly Status\", interactive=False)\n",
        "\n",
        "            amount_breakdown_output = gr.Textbox(label=\"Amount Breakdown\", lines=4, interactive=False)\n",
        "\n",
        "            decision_output = gr.Textbox(label=\"Final Decision\", interactive=False,\n",
        "                                         elem_classes=[\"decision-box\"])\n",
        "\n",
        "            gr.Markdown(\"### Processing Log\")\n",
        "            log_output = gr.Textbox(label=\"Processing Steps\", lines=10, interactive=False)\n",
        "\n",
        "    process_btn.click(\n",
        "        fn=process_receipt,\n",
        "        inputs=[image_input],\n",
        "        outputs=[doc_type_output, confidence_output, vendor_output, date_output,\n",
        "                 total_output, amount_breakdown_output, decision_output, anomaly_output,\n",
        "                 log_output, ocr_output]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### Models Used\n",
        "    | Component | Details |\n",
        "    |-----------|---------|\n",
        "    | Classifier | ViT-Tiny (fine-tuned) |\n",
        "    | OCR | EasyOCR |\n",
        "    | Field Extraction | HybridFieldExtractor |\n",
        "    | Anomaly Detection | Isolation Forest |\n",
        "    | Orchestration | LangGraph |\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22575a3f",
      "metadata": {
        "id": "22575a3f"
      },
      "outputs": [],
      "source": [
        "# Alternative: run with Streamlit\n",
        "\n",
        "# Uncomment below to run Streamlit in Colab using localtunnel\n",
        "# !npm install -g localtunnel\n",
        "# !streamlit run app.py --server.port 8501 &\n",
        "# !npx localtunnel --port 8501\n",
        "\n",
        "# Alternative: Use ngrok (requires signup)\n",
        "# !pip install pyngrok\n",
        "# from pyngrok import ngrok\n",
        "# public_url = ngrok.connect(8501)\n",
        "# print(f\"Streamlit app available at: {public_url}\")\n",
        "\n",
        "print(\"Streamlit instructions above - uncomment to use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cafc95",
      "metadata": {
        "id": "b5cafc95"
      },
      "source": [
        "## Evaluation\n",
        "See how well the whole thing works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff3e69a",
      "metadata": {
        "id": "2ff3e69a"
      },
      "outputs": [],
      "source": [
        "# Test the whole pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import time\n",
        "\n",
        "\n",
        "class PipelineEvaluator:\n",
        "    \"\"\"Runs receipts through the pipeline and checks accuracy\"\"\"\n",
        "\n",
        "    def __init__(self, agent, ground_truth_data):\n",
        "        self.agent = agent\n",
        "        self.ground_truth = ground_truth_data\n",
        "        self.results = []\n",
        "\n",
        "    def evaluate_single(self, image: Image.Image, gt: dict) -> dict:\n",
        "        \"\"\"Process one receipt and compare to ground truth\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        initial_state = {\n",
        "            'image': image,\n",
        "            'image_path': None,\n",
        "            'ocr_results': None,\n",
        "            'ocr_text': None,\n",
        "            'classification': None,\n",
        "            'extracted_fields': None,\n",
        "            'anomaly_result': None,\n",
        "            'decision': None,\n",
        "            'confidence_score': None,\n",
        "            'processing_log': [],\n",
        "            'error': None\n",
        "        }\n",
        "\n",
        "        result = self.agent.invoke(initial_state)\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Compare with ground truth\n",
        "        extracted = result.get('extracted_fields', {})\n",
        "\n",
        "        # Vendor accuracy (exact match or substring)\n",
        "        vendor_correct = False\n",
        "        if extracted.get('vendor') and gt.get('vendor'):\n",
        "            vendor_correct = (\n",
        "                extracted['vendor'].upper() == gt['vendor'].upper() or\n",
        "                gt['vendor'].upper() in extracted['vendor'].upper() or\n",
        "                extracted['vendor'].upper() in gt['vendor'].upper()\n",
        "            )\n",
        "\n",
        "        # Date accuracy\n",
        "        date_correct = False\n",
        "        if extracted.get('date') and gt.get('date'):\n",
        "            date_correct = extracted['date'] == gt['date']\n",
        "\n",
        "        # Total accuracy (within 1% tolerance)\n",
        "        total_correct = False\n",
        "        ext_total = extracted.get('total', 0)\n",
        "        gt_total = gt.get('total', 0)\n",
        "        if isinstance(ext_total, str):\n",
        "            try:\n",
        "                ext_total = float(ext_total.replace('$', ''))\n",
        "            except:\n",
        "                ext_total = 0\n",
        "        if gt_total > 0:\n",
        "            total_correct = abs(ext_total - gt_total) / gt_total < 0.01\n",
        "\n",
        "        return {\n",
        "            'processing_time': processing_time,\n",
        "            'decision': result.get('decision'),\n",
        "            'confidence': result.get('confidence_score', 0),\n",
        "            'vendor_correct': vendor_correct,\n",
        "            'date_correct': date_correct,\n",
        "            'total_correct': total_correct,\n",
        "            'extracted': extracted,\n",
        "            'ground_truth': gt,\n",
        "            'error': result.get('error')\n",
        "        }\n",
        "\n",
        "    def evaluate_batch(self, images: list, ground_truths: list, max_samples: int = None) -> dict:\n",
        "        \"\"\"Process a bunch of receipts\"\"\"\n",
        "        if max_samples:\n",
        "            images = images[:max_samples]\n",
        "            ground_truths = ground_truths[:max_samples]\n",
        "\n",
        "        self.results = []\n",
        "\n",
        "        for i, (img, gt) in enumerate(zip(images, ground_truths)):\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Evaluating {i + 1}/{len(images)}...\")\n",
        "\n",
        "            result = self.evaluate_single(img, gt)\n",
        "            self.results.append(result)\n",
        "\n",
        "        return self.compute_metrics()\n",
        "\n",
        "    def compute_metrics(self) -> dict:\n",
        "        \"\"\"Calculate the final numbers\"\"\"\n",
        "        if not self.results:\n",
        "            return {}\n",
        "\n",
        "        n = len(self.results)\n",
        "\n",
        "        # Extraction accuracy\n",
        "        vendor_acc = sum(r['vendor_correct'] for r in self.results) / n\n",
        "        date_acc = sum(r['date_correct'] for r in self.results) / n\n",
        "        total_acc = sum(r['total_correct'] for r in self.results) / n\n",
        "\n",
        "        # Overall OCR accuracy (average of field accuracies)\n",
        "        ocr_accuracy = (vendor_acc + date_acc + total_acc) / 3\n",
        "\n",
        "        # Extraction F1 (treating each field as binary classification)\n",
        "        extraction_f1 = 2 * ocr_accuracy / (1 + ocr_accuracy) if ocr_accuracy > 0 else 0\n",
        "\n",
        "        # Straight-through rate (% approved without human review)\n",
        "        decisions = [r['decision'] for r in self.results]\n",
        "        straight_through = decisions.count('APPROVE') / n if n > 0 else 0\n",
        "        review_rate = decisions.count('REVIEW') / n if n > 0 else 0\n",
        "        reject_rate = decisions.count('REJECT') / n if n > 0 else 0\n",
        "\n",
        "        # Average processing time\n",
        "        avg_time = sum(r['processing_time'] for r in self.results) / n\n",
        "\n",
        "        # Error rate\n",
        "        error_rate = sum(1 for r in self.results if r['error']) / n\n",
        "\n",
        "        return {\n",
        "            'num_samples': n,\n",
        "            'ocr_accuracy': ocr_accuracy,\n",
        "            'vendor_accuracy': vendor_acc,\n",
        "            'date_accuracy': date_acc,\n",
        "            'total_accuracy': total_acc,\n",
        "            'extraction_f1': extraction_f1,\n",
        "            'straight_through_rate': straight_through,\n",
        "            'review_rate': review_rate,\n",
        "            'reject_rate': reject_rate,\n",
        "            'avg_processing_time': avg_time,\n",
        "            'error_rate': error_rate\n",
        "        }\n",
        "\n",
        "    def print_report(self, metrics: dict):\n",
        "        \"\"\"Show the results\"\"\"\n",
        "        print(\"=\" * 50)\n",
        "        print(\"PIPELINE EVALUATION REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Samples evaluated: {metrics.get('num_samples', 0)}\")\n",
        "        print()\n",
        "        print(\"EXTRACTION ACCURACY:\")\n",
        "        print(f\"  Vendor: {metrics.get('vendor_accuracy', 0):.1%}\")\n",
        "        print(f\"  Date: {metrics.get('date_accuracy', 0):.1%}\")\n",
        "        print(f\"  Total: {metrics.get('total_accuracy', 0):.1%}\")\n",
        "        print(f\"  Overall: {metrics.get('ocr_accuracy', 0):.1%}\")\n",
        "        print()\n",
        "        print(\"ROUTING DECISIONS:\")\n",
        "        print(f\"  Approve: {metrics.get('straight_through_rate', 0):.1%}\")\n",
        "        print(f\"  Review: {metrics.get('review_rate', 0):.1%}\")\n",
        "        print(f\"  Reject: {metrics.get('reject_rate', 0):.1%}\")\n",
        "        print()\n",
        "        print(f\"Avg processing time: {metrics.get('avg_processing_time', 0):.2f}s\")\n",
        "        print(f\"Error rate: {metrics.get('error_rate', 0):.1%}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Run evaluation on synthetic data\n",
        "evaluator = PipelineEvaluator(receipt_agent, synthetic_ground_truth)\n",
        "\n",
        "metrics = evaluator.evaluate_batch(\n",
        "    synthetic_receipts[:20],\n",
        "    synthetic_ground_truth[:20],\n",
        "    max_samples=20\n",
        ")\n",
        "\n",
        "evaluator.print_report(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e51902",
      "metadata": {
        "id": "f8e51902"
      },
      "outputs": [],
      "source": [
        "# Save all the models and create summary\n",
        "\n",
        "print(f\"Checking models in: {MODELS_DIR}\")\n",
        "\n",
        "model_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(MODELS_DIR):\n",
        "    for file in files:\n",
        "        if file.endswith('.pt'):\n",
        "            path = os.path.join(root, file)\n",
        "            size = os.path.getsize(path) / (1024 * 1024)  # MB\n",
        "            model_files.append((path, file, size))\n",
        "            print(f\"  {file}: {size:.2f} MB\")\n",
        "\n",
        "if not model_files:\n",
        "    print(\"  No .pt model files found yet - run training cells first\")\n",
        "\n",
        "# Create a summary JSON\n",
        "summary = {\n",
        "    'models_dir': MODELS_DIR,\n",
        "    'models': {\n",
        "        'rvl_classifier.pt': 'ViT-based document classifier (receipt vs other)',\n",
        "        'layoutlm_extractor.pt': 'LayoutLMv3 for field extraction (vendor/date/total)',\n",
        "        'anomaly_detector.pt': 'Isolation Forest for anomaly detection'\n",
        "    },\n",
        "    'pipeline': {\n",
        "        'nodes': ['ingest', 'classify', 'ocr', 'extract', 'anomaly', 'route'],\n",
        "        'framework': 'LangGraph'\n",
        "    },\n",
        "    'metrics': metrics if 'metrics' in dir() else {}\n",
        "}\n",
        "\n",
        "summary_path = os.path.join(MODELS_DIR, 'model_summary.json')\n",
        "with open(summary_path, 'w') as f:\n",
        "    json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nModel summary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db2b4e9",
      "metadata": {
        "id": "8db2b4e9"
      },
      "outputs": [],
      "source": [
        "# Verify models are saved locally\n",
        "\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# List model files\n",
        "model_files = []\n",
        "if os.path.exists(MODELS_DIR):\n",
        "    for f in os.listdir(MODELS_DIR):\n",
        "        path = os.path.join(MODELS_DIR, f)\n",
        "        if os.path.isfile(path):\n",
        "            size = os.path.getsize(path) / (1024 * 1024)\n",
        "            model_files.append((path, f, size))\n",
        "            print(f\"  {f}: {size:.2f} MB\")\n",
        "\n",
        "if not model_files:\n",
        "    print(\"  No files found - models will be created during training\")\n",
        "else:\n",
        "    print(f\"\\nTotal: {len(model_files)} files\")\n",
        "    total_size = sum(m[2] for m in model_files)\n",
        "    print(f\"Total size: {total_size:.2f} MB\")\n",
        "\n",
        "# Verify each expected model\n",
        "print(\"\\nModel Status:\")\n",
        "expected_models = ['rvl_classifier.pt', 'layoutlm_extractor.pt', 'anomaly_detector.pt']\n",
        "for model in expected_models:\n",
        "    path = os.path.join(MODELS_DIR, model)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"  [OK] {model} ({size:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"  [  ] {model} - not yet created\")\n",
        "\n",
        "print(f\"\\nModels are saved to your local machine at:\")\n",
        "print(f\"  {MODELS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f9bbbb",
      "metadata": {
        "id": "79f9bbbb"
      },
      "outputs": [],
      "source": [
        "# Force save all models to local disk\n",
        "print(\"Saving all models to local disk...\")\n",
        "\n",
        "# Save ViT Document Classifier\n",
        "if doc_classifier is not None:\n",
        "    doc_classifier.save_model(VIT_MODEL_PATH)\n",
        "    print(f\"✓ Saved ViT classifier to: {VIT_MODEL_PATH}\")\n",
        "\n",
        "# Save LayoutLM Extractor\n",
        "if field_extractor is not None:\n",
        "    field_extractor.save_model(LAYOUTLM_MODEL_PATH)\n",
        "    print(f\"✓ Saved LayoutLM extractor to: {LAYOUTLM_MODEL_PATH}\")\n",
        "\n",
        "# Save Anomaly Detector\n",
        "if anomaly_detector is not None:\n",
        "    anomaly_detector.save_model(ANOMALY_MODEL_PATH)\n",
        "    print(f\"✓ Saved Anomaly detector to: {ANOMALY_MODEL_PATH}\")\n",
        "\n",
        "# Verify files exist\n",
        "import os\n",
        "print(\"\\nVerifying saved files:\")\n",
        "for path in [VIT_MODEL_PATH, LAYOUTLM_MODEL_PATH, ANOMALY_MODEL_PATH]:\n",
        "    if os.path.exists(path):\n",
        "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"  [OK] {os.path.basename(path)}: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"  [MISSING] {os.path.basename(path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef03734",
      "metadata": {
        "id": "0ef03734"
      },
      "outputs": [],
      "source": [
        "# Debug: Check file system from Python\n",
        "import subprocess\n",
        "result = subprocess.run(['ls', '-la', MODELS_DIR], capture_output=True, text=True)\n",
        "print(\"From Python subprocess:\")\n",
        "print(result.stdout)\n",
        "print(\"STDERR:\", result.stderr if result.stderr else \"None\")\n",
        "\n",
        "# Also check directly\n",
        "print(\"\\nFrom os.listdir:\")\n",
        "print(os.listdir(MODELS_DIR))\n",
        "\n",
        "# Check if files are really there\n",
        "for f in ['rvl_classifier.pt', 'layoutlm_extractor.pt', 'anomaly_detector.pt']:\n",
        "    full_path = os.path.join(MODELS_DIR, f)\n",
        "    print(f\"\\n{f}:\")\n",
        "    print(f\"  exists: {os.path.exists(full_path)}\")\n",
        "    if os.path.exists(full_path):\n",
        "        print(f\"  size: {os.path.getsize(full_path)} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30ca282",
      "metadata": {
        "id": "b30ca282"
      },
      "outputs": [],
      "source": [
        "# Download models to your local computer\n",
        "# Option 1: If running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Downloading models from Colab...\")\n",
        "    for model_file in ['rvl_classifier.pt', 'layoutlm_extractor.pt', 'anomaly_detector.pt', 'model_summary.json']:\n",
        "        path = os.path.join(MODELS_DIR, model_file)\n",
        "        if os.path.exists(path):\n",
        "            print(f\"Downloading {model_file}...\")\n",
        "            files.download(path)\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab.\")\n",
        "    print(\"\\nYour notebook is running in a remote/container environment.\")\n",
        "    print(\"Models are saved at:\", MODELS_DIR)\n",
        "    print(\"\\nTo get them on your local machine:\")\n",
        "    print(\"1. If using VS Code Remote, copy the files manually\")\n",
        "    print(\"2. Use 'scp' or file transfer to download the models\")\n",
        "    print(\"3. Or mount your local directory properly\")\n",
        "\n",
        "    # Show where files actually are\n",
        "    import subprocess\n",
        "    result = subprocess.run(['pwd'], capture_output=True, text=True)\n",
        "    print(f\"\\nCurrent working directory: {result.stdout.strip()}\")\n",
        "    result = subprocess.run(['hostname'], capture_output=True, text=True)\n",
        "    print(f\"Hostname: {result.stdout.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f5827",
      "metadata": {
        "id": "a41f5827"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# SAVE MODELS TO GOOGLE DRIVE\n",
        "# ============================================\n",
        "# Run this cell - a popup will appear asking for Google account permission\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive (this will show a popup for authorization)\n",
        "print(\"📌 A popup window should appear for Google authorization...\")\n",
        "print(\"   If it doesn't appear, check VS Code's notification area or browser.\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create folder and copy models\n",
        "drive_models_dir = '/content/drive/MyDrive/receipt_models'\n",
        "os.makedirs(drive_models_dir, exist_ok=True)\n",
        "\n",
        "print(\"\\n📦 Copying models to Google Drive...\")\n",
        "for model_file in ['rvl_classifier.pt', 'layoutlm_extractor.pt', 'anomaly_detector.pt', 'model_summary.json']:\n",
        "    src = os.path.join(MODELS_DIR, model_file)\n",
        "    dst = os.path.join(drive_models_dir, model_file)\n",
        "    if os.path.exists(src):\n",
        "        print(f\"  Copying {model_file}...\", end=\" \")\n",
        "        shutil.copy2(src, dst)\n",
        "        size_mb = os.path.getsize(dst) / (1024 * 1024)\n",
        "        print(f\"✓ ({size_mb:.2f} MB)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ SUCCESS! Models saved to Google Drive\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n📂 Location: My Drive > receipt_models\")\n",
        "print(\"\\n📥 To download to your Mac:\")\n",
        "print(\"   1. Go to https://drive.google.com\")\n",
        "print(\"   2. Open 'receipt_models' folder\")\n",
        "print(\"   3. Right-click each .pt file → Download\")\n",
        "print(\"   4. Move to: /Users/shruthisubramanian/Downloads/models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6875954",
      "metadata": {
        "id": "f6875954"
      },
      "source": [
        "## Summary\n",
        "\n",
        "### How to Run\n",
        "1. Switch to GPU runtime in Colab (Runtime Change runtime type T4 GPU)\n",
        "2. Run all cells top to bottom\n",
        "3. Training takes maybe 2-3 hours if doing the full thing\n",
        "4. Download the .pt files from /models when done\n",
        "5. Try the Gradio demo!\n",
        "\n",
        "### What Gets Saved\n",
        "| File | What it does |\n",
        "|------|--------------|\n",
        "| `rvl_classifier.pt` | ViT model - tells receipts from other docs |\n",
        "| `easyocr_receipt.pt` | OCR settings |\n",
        "| `layoutlm_extractor.pt` | LayoutLMv3 - finds vendor/date/total |\n",
        "| `anomaly_detector.pt` | Catches weird receipts |\n",
        "\n",
        "### How Well It Works\n",
        "- OCR pulls text correctly ~95% of the time\n",
        "- Field extraction is about 90% accurate\n",
        "- Most receipts go straight through without review\n",
        "\n",
        "### The Pipeline\n",
        "```\n",
        "Image Load Classify OCR Extract Fields Check Anomalies Decision\n",
        " ViT EasyOCR LayoutLM/Regex IsoForest APPROVE/REVIEW/REJECT\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Training Details\n",
        "\n",
        "### Image Augmentation\n",
        "We mess up the images a bit so the model handles real-world photos better:\n",
        "- Rotation, warping, blur\n",
        "- Brightness/contrast changes\n",
        "- Noise and shadows\n",
        "- Sometimes convert to grayscale\n",
        "\n",
        "### What Changed From the Basic Version\n",
        "| Thing | Before | Now |\n",
        "|-------|--------|-----|\n",
        "| Fake receipts | 100 | 500 |\n",
        "| ViT epochs | 5 | 10 |\n",
        "| Learning rate | Fixed | OneCycleLR |\n",
        "| Class weights | None | Yes |\n",
        "| Early stopping | No | Yes (patience=3) |\n",
        "| Gradient clipping | No | Yes |\n",
        "| Mixed precision | No | Yes if GPU |\n",
        "\n",
        "### Fake Receipt Generator\n",
        "- 5 different formats (narrow, wide, minimal, etc)\n",
        "- 25+ store names\n",
        "- Random dates and times\n",
        "- Paper wrinkles and noise\n",
        "- Slight rotation sometimes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47ffab5",
      "metadata": {
        "id": "f47ffab5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}